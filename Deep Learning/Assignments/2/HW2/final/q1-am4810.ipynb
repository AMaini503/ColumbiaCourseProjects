{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import copy\n",
    "from time import time\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)  # for Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF order [batch, in_height, in_width, in_channels]\n",
    "## opencv,scipy H W K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)\n",
    "        \n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_one_hot[i,y[i]]=1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    X = np.zeros((len(files),32,32,3))\n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = scipy.misc.imread(f)\n",
    "        #img_arr = img_arr.flatten() / 255.0\n",
    "        img_arr = img_arr.astype(float)  #H,W,K\n",
    "        images.append(img_arr)\n",
    "        X[count-1,:,:,:] = img_arr[np.newaxis,:]   #  [batch, in_height, in_width, in_channels]\n",
    "\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)\n",
    "\n",
    "def augment(X):\n",
    "    #(batch_size, 32, 32, 3)\n",
    "    for i in range(X.shape[0]):\n",
    "        img = X[i,:,:,:]\n",
    "        flip_p = random.uniform(0, 1)\n",
    "        flip_crop = random.uniform(0, 1)\n",
    "        if flip_p > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "        \n",
    "        \n",
    "        if flip_crop > 0.5:\n",
    "            crop_x = np.random.randint(5, size=1)\n",
    "            crop_y = np.random.randint(5, size=1)\n",
    "            img = img[crop_x[0]:crop_x[0]+28,crop_y[0]:crop_y[0]+28,:]\n",
    "            img = scipy.misc.imresize(img,(32,32))\n",
    "            \n",
    "        X[i,:,:,:] = img\n",
    "    return X\n",
    "    \n",
    "def get_batch(X, y, batch_size,list_,counter):\n",
    "    \"\"\"\n",
    "    Return minibatch of samples and labels\n",
    "\n",
    "    :param X, y: samples and corresponding labels\n",
    "    :parma batch_size: minibatch size\n",
    "    :returns: (tuple) X_batch, y_batch\n",
    "    \"\"\"\n",
    "    idx = list_[counter:counter+batch_size]\n",
    "    X_batch = X[idx,:,:,:]\n",
    "    y_batch = y[idx]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 7, 'automobile': 1, 'deer': 4, 'dog': 5, 'frog': 6, 'cat': 3, 'truck': 9, 'ship': 8, 'airplane': 0, 'bird': 2}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input normalization\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "X_train_stddev = np.std(X_train,axis=0)\n",
    "X_train = (X_train-X_train_mean)/X_train_stddev\n",
    "X_test = (X_test-X_train_mean)/X_train_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_max=20000\n",
    "batch_size=100\n",
    "split_ratio = 0.9\n",
    "lr_ = 0.01\n",
    "orig_full = range(len(y_train))\n",
    "np.random.shuffle(orig_full)\n",
    "split_idx = int((len(orig_full)*split_ratio))\n",
    "orig_train = orig_full[:split_idx]\n",
    "orig_val = orig_full[split_idx:]\n",
    "counter = 0\n",
    "train_idx = []\n",
    "epocs = (iters_max*batch_size)/len(orig_train) + 1\n",
    "for i in range(epocs):\n",
    "    np.random.shuffle(orig_train)\n",
    "    train_idx.extend(orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1\n",
    "images = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "gt = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "lr = tf.placeholder(tf.float32, shape=())\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    init = tf.truncated_normal_initializer(stddev=1e-2, dtype=tf.float32)\n",
    "    kernel = tf.get_variable('weights', shape=[5, 5, 3, 64], initializer=init, dtype=tf.float32)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    \n",
    "# pool1\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                     padding='SAME', name='pool1')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1') \n",
    "# conv2\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    init = tf.truncated_normal_initializer(stddev=1e-2, dtype=tf.float32)\n",
    "    kernel = tf.get_variable('weights', shape=[5, 5, 64, 64], initializer=init, dtype=tf.float32)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    \n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                   padding='SAME', name='pool2')\n",
    "norm2 = tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2') \n",
    " \n",
    "\n",
    "with tf.variable_scope('FC1') as scope:\n",
    "    reshape = tf.reshape(norm2, [batch_size, -1])\n",
    "    init = tf.truncated_normal_initializer(stddev=1/4096.0, dtype=tf.float32)\n",
    "    weights = tf.get_variable('weights', shape=[4096,10], initializer=init, dtype=tf.float32)\n",
    "    biases = tf.get_variable('biases', shape=[10], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    before_softmax = tf.add(tf.matmul(reshape, weights), biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = before_softmax, labels = gt)\n",
    "loss = tf.reduce_mean(loss)\n",
    "correct_prediction_df = tf.equal(tf.argmax(before_softmax,1), tf.argmax(gt,1))\n",
    "accuracy_df = tf.reduce_mean(tf.cast(correct_prediction_df, tf.float32))\n",
    "train_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('train_acc',accuracy_df)\n",
    "conv1_grad = tf.gradients(loss, [conv1])[0]\n",
    "conv2_grad = tf.gradients(loss, [conv2])[0]\n",
    "fc1_grad = tf.gradients(loss, [before_softmax])[0]\n",
    "tf.summary.histogram('conv1_grad',conv1_grad)\n",
    "tf.summary.histogram('conv2_grad',conv2_grad)\n",
    "tf.summary.histogram('fc1_grad',fc1_grad)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('tensorboard/')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "best_till_now = 0.0\n",
    "best_train = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.30106\n",
      "val accuracy  0.097 train accuracy  0.100333\n",
      "100 2.001\n",
      "200 1.84705\n",
      "300 1.48825\n",
      "400 1.53309\n",
      "500 1.54517\n",
      "val accuracy  0.4752 train accuracy  0.464689\n",
      "600 1.49252\n",
      "700 1.47902\n",
      "800 1.44933\n",
      "900 1.45906\n",
      "1000 1.2392\n",
      "val accuracy  0.5574 train accuracy  0.559533\n",
      "1100 1.23951\n",
      "1200 1.1011\n",
      "1300 1.3793\n",
      "1400 1.12858\n",
      "1500 1.21049\n",
      "val accuracy  0.601 train accuracy  0.618267\n",
      "1600 1.31031\n",
      "1700 1.03034\n",
      "1800 0.873924\n",
      "1900 1.04275\n",
      "2000 1.01705\n",
      "val accuracy  0.6018 train accuracy  0.625778\n",
      "2100 0.896921\n",
      "2200 1.28447\n",
      "2300 0.926483\n",
      "2400 0.860184\n",
      "2500 1.02318\n",
      "val accuracy  0.6562 train accuracy  0.690778\n",
      "2600 1.07583\n",
      "2700 0.795625\n",
      "2800 0.899638\n",
      "2900 0.882655\n",
      "3000 1.04379\n",
      "val accuracy  0.6658 train accuracy  0.698578\n",
      "3100 1.00066\n",
      "3200 0.685729\n",
      "3300 0.892168\n",
      "3400 0.675609\n",
      "3500 1.00776\n",
      "val accuracy  0.6812 train accuracy  0.715755\n",
      "3600 0.641563\n",
      "3700 0.827626\n",
      "3800 0.90234\n",
      "3900 0.746109\n",
      "4000 0.913354\n",
      "val accuracy  0.682 train accuracy  0.726755\n",
      "4100 0.807102\n",
      "4200 0.614357\n",
      "4300 0.686105\n",
      "4400 0.964341\n",
      "4500 0.694777\n",
      "val accuracy  0.694 train accuracy  0.750422\n",
      "4600 0.807902\n",
      "4700 0.829976\n",
      "4800 0.714591\n",
      "4900 0.679056\n",
      "5000 0.675888\n",
      "val accuracy  0.6972 train accuracy  0.761133\n",
      "5100 0.893935\n",
      "5200 0.66326\n",
      "5300 0.65752\n",
      "5400 0.611996\n",
      "5500 0.622008\n",
      "val accuracy  0.682 train accuracy  0.739822\n",
      "5600 0.523051\n",
      "5700 0.80003\n",
      "5800 0.731886\n",
      "5900 0.690261\n",
      "6000 0.711546\n",
      "val accuracy  0.704 train accuracy  0.775067\n",
      "6100 0.704396\n",
      "6200 0.881188\n",
      "6300 0.918073\n",
      "6400 0.80971\n",
      "6500 0.565067\n",
      "val accuracy  0.7154 train accuracy  0.7904\n",
      "6600 0.570073\n",
      "6700 0.576331\n",
      "6800 0.67372\n",
      "6900 0.891979\n",
      "7000 0.663353\n",
      "val accuracy  0.7146 train accuracy  0.803689\n",
      "7100 0.871593\n",
      "7200 0.840775\n",
      "7300 0.604696\n",
      "7400 0.629991\n",
      "7500 0.859031\n",
      "val accuracy  0.699 train accuracy  0.790267\n",
      "7600 0.676311\n",
      "7700 0.733185\n",
      "7800 0.583417\n",
      "7900 0.611587\n",
      "8000 0.637136\n",
      "val accuracy  0.7146 train accuracy  0.809867\n",
      "8100 0.664288\n",
      "8200 0.557191\n",
      "8300 0.739395\n",
      "8400 0.421856\n",
      "8500 0.642798\n",
      "val accuracy  0.7198 train accuracy  0.820622\n",
      "8600 0.537666\n",
      "8700 0.463584\n",
      "8800 0.516386\n",
      "8900 0.693521\n",
      "9000 0.534413\n",
      "val accuracy  0.7222 train accuracy  0.824489\n",
      "9100 0.676176\n",
      "9200 0.556003\n",
      "9300 0.451422\n",
      "9400 0.381304\n",
      "9500 0.666759\n",
      "val accuracy  0.6968 train accuracy  0.783644\n",
      "9600 0.563191\n",
      "9700 0.819228\n",
      "9800 0.583174\n",
      "9900 0.556343\n",
      "10000 0.328384\n",
      "val accuracy  0.7196 train accuracy  0.840178\n",
      "10100 0.488506\n",
      "10200 0.344731\n",
      "10300 0.492555\n",
      "10400 0.483769\n",
      "10500 0.459132\n",
      "val accuracy  0.7356 train accuracy  0.863178\n",
      "10600 0.337723\n",
      "10700 0.453984\n",
      "10800 0.418422\n",
      "10900 0.491658\n",
      "11000 0.401519\n",
      "val accuracy  0.739 train accuracy  0.866267\n",
      "11100 0.511676\n",
      "11200 0.396377\n",
      "11300 0.512299\n",
      "11400 0.4091\n",
      "11500 0.287373\n",
      "val accuracy  0.7382 train accuracy  0.867133\n",
      "11600 0.37469\n",
      "11700 0.323318\n",
      "11800 0.435248\n",
      "11900 0.474947\n",
      "12000 0.371856\n",
      "val accuracy  0.7382 train accuracy  0.8684\n",
      "12100 0.458756\n",
      "12200 0.434681\n",
      "12300 0.376778\n",
      "12400 0.393104\n",
      "12500 0.410002\n",
      "val accuracy  0.7382 train accuracy  0.867733\n",
      "12600 0.300183\n",
      "12700 0.514121\n",
      "12800 0.43543\n",
      "12900 0.349869\n",
      "13000 0.580178\n",
      "val accuracy  0.7376 train accuracy  0.868844\n",
      "13100 0.346424\n",
      "13200 0.363843\n",
      "13300 0.38771\n",
      "13400 0.406774\n",
      "13500 0.40155\n",
      "val accuracy  0.7384 train accuracy  0.870422\n",
      "13600 0.373558\n",
      "13700 0.395757\n",
      "13800 0.428527\n",
      "13900 0.406054\n",
      "14000 0.437025\n",
      "val accuracy  0.7374 train accuracy  0.871844\n",
      "14100 0.364268\n",
      "14200 0.389241\n",
      "14300 0.371261\n",
      "14400 0.336382\n",
      "14500 0.427708\n",
      "val accuracy  0.7384 train accuracy  0.871689\n",
      "14600 0.291844\n",
      "14700 0.595084\n",
      "14800 0.427908\n",
      "14900 0.445789\n",
      "15000 0.467992\n",
      "val accuracy  0.7378 train accuracy  0.8734\n",
      "15100 0.369238\n",
      "15200 0.354551\n",
      "15300 0.466581\n",
      "15400 0.418148\n",
      "15500 0.401337\n",
      "val accuracy  0.7374 train accuracy  0.873289\n",
      "15600 0.352512\n",
      "15700 0.475635\n",
      "15800 0.428158\n",
      "15900 0.391334\n",
      "16000 0.41555\n",
      "val accuracy  0.7408 train accuracy  0.874289\n",
      "16100 0.365216\n",
      "16200 0.535426\n",
      "16300 0.263008\n",
      "16400 0.334878\n",
      "16500 0.448355\n",
      "val accuracy  0.7442 train accuracy  0.876956\n",
      "16600 0.468823\n",
      "16700 0.544338\n",
      "16800 0.428832\n",
      "16900 0.359084\n",
      "17000 0.33458\n",
      "val accuracy  0.7396 train accuracy  0.876067\n",
      "17100 0.436384\n",
      "17200 0.452319\n",
      "17300 0.445134\n",
      "17400 0.484909\n",
      "17500 0.317321\n",
      "val accuracy  0.7398 train accuracy  0.877933\n",
      "17600 0.310486\n",
      "17700 0.335597\n",
      "17800 0.490621\n",
      "17900 0.514036\n",
      "18000 0.257007\n",
      "val accuracy  0.736 train accuracy  0.879289\n",
      "18100 0.451279\n",
      "18200 0.483458\n",
      "18300 0.470742\n",
      "18400 0.518131\n",
      "18500 0.556798\n",
      "val accuracy  0.7402 train accuracy  0.878489\n",
      "18600 0.42616\n",
      "18700 0.358836\n",
      "18800 0.437369\n",
      "18900 0.348079\n",
      "19000 0.329248\n",
      "val accuracy  0.7388 train accuracy  0.879755\n",
      "19100 0.342726\n",
      "19200 0.402919\n",
      "19300 0.405673\n",
      "19400 0.289207\n",
      "19500 0.465913\n",
      "val accuracy  0.7366 train accuracy  0.880711\n",
      "19600 0.409323\n",
      "19700 0.435362\n",
      "19800 0.293143\n",
      "19900 0.415211\n"
     ]
    }
   ],
   "source": [
    "with  tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for iter in range(iters_max):\n",
    "        X_batch, y_batch = get_batch( X_train, y_train, batch_size,train_idx,counter)\n",
    "        counter += batch_size\n",
    "        y_batch = one_hot(y_batch).transpose((1,0))\n",
    "        #loss.run(feed_dict={images: X_batch, gt: y_batch})\n",
    "        if iter%10000 == 1:\n",
    "            lr_/=10.0\n",
    "        _, loss_npy,summary = sess.run([train_step,loss,merged],feed_dict={images: X_batch, gt: y_batch,lr:lr_})\n",
    "        \n",
    "\n",
    "        if iter%100==0:\n",
    "            print iter, loss_npy\n",
    "        if iter%500==0:\n",
    "            test_counter = 0\n",
    "            train_counter = 0\n",
    "            acc_val  = []\n",
    "            acc_train  = []\n",
    "            while test_counter < len(orig_val):  # check if we need to add +1 or -1 REMOVE this BUG\n",
    "                X_batch, y_batch = get_batch( X_train, y_train, batch_size,orig_val,test_counter)\n",
    "                y_batch = one_hot(y_batch).transpose((1,0))\n",
    "                test_counter+=batch_size\n",
    "                acc_val.append(sess.run([accuracy_df],feed_dict={images: X_batch, gt: y_batch,lr:lr_}))\n",
    "            while train_counter < len(orig_train):  # check if we need to add +1 or -1 REMOVE this BUG\n",
    "                X_batch, y_batch = get_batch( X_train, y_train, batch_size,orig_train,train_counter)\n",
    "                y_batch = one_hot(y_batch).transpose((1,0))\n",
    "                train_counter+=batch_size\n",
    "                acc_train.append(sess.run([accuracy_df],feed_dict={images: X_batch, gt: y_batch, lr:lr_}))\n",
    "            print 'val accuracy ', np.mean(np.asarray(acc_val)),'train accuracy ', np.mean(np.asarray(acc_train))\n",
    "            if np.mean(np.asarray(acc_val)) > best_till_now:\n",
    "                best_till_now = np.mean(np.asarray(acc_val))\n",
    "                best_train = np.mean(np.asarray(acc_train))\n",
    "                saver.save(sess, 'part1/best-val-model-part1')\n",
    "            train_acc = np.mean(np.asarray(acc_train))\n",
    "        train_writer.add_summary(summary, iter)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val accuracy -  0.7442 best train accuracy 0.876956\n"
     ]
    }
   ],
   "source": [
    "print 'best val accuracy - ',best_till_now, 'best train accuracy',best_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.import_meta_graph('part1/best-val-model-part1.meta')\n",
    "    saver.restore(sess, \"part1/best-val-model-part1\")\n",
    "    test_counter = 0\n",
    "    y_pred = np.zeros((10,X_test.shape[0]))\n",
    "    count = 0\n",
    "    test_order_list = range(X_test.shape[0])\n",
    "    y_test = np.zeros((X_test.shape[0]),dtype=np.int) #fake y_test values, will not be used\n",
    "    while test_counter < len(test_order_list): \n",
    "        X_batch, y_batch = get_batch( X_test, y_test, batch_size,test_order_list,test_counter)\n",
    "        y_batch = one_hot(y_batch).transpose((1,0))\n",
    "        test_counter+=batch_size\n",
    "        batch_scores = sess.run([before_softmax],feed_dict={images: X_batch, gt: y_batch,lr:lr_})\n",
    "        y_pred[:,count:count+batch_size] = batch_scores[0].transpose((1,0))\n",
    "        count+=batch_size\n",
    "\n",
    "    np.save('part1/ans1-uni.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 4213.69451594\n"
     ]
    }
   ],
   "source": [
    "t2 = time()\n",
    "print 'time taken', t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
