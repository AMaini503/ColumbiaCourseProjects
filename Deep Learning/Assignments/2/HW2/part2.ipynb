{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import copy\n",
    "from time import time\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF order [batch, in_height, in_width, in_channels]\n",
    "## scipy H W K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)\n",
    "        \n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_one_hot[i,y[i]]=1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    X = np.zeros((len(files),32,32,3))\n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = scipy.misc.imread(f)\n",
    "        #img_arr = img_arr.flatten() / 255.0\n",
    "        img_arr = img_arr.astype(float)  #H,W,K\n",
    "        images.append(img_arr)\n",
    "        X[count-1,:,:,:] = img_arr[np.newaxis,:]   #  [batch, in_height, in_width, in_channels]\n",
    "\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)\n",
    "\n",
    "def augment(X):\n",
    "    #(batch_size, 32, 32, 3)\n",
    "    for i in range(X.shape[0]):\n",
    "        img = X[i,:,:,:]\n",
    "        flip_p = random.uniform(0, 1)\n",
    "        flip_crop = random.uniform(0, 1)\n",
    "        if flip_p > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "        \n",
    "        \n",
    "        if flip_crop > 0.5:\n",
    "            crop_x = np.random.randint(5, size=1)\n",
    "            crop_y = np.random.randint(5, size=1)\n",
    "            img = img[crop_x[0]:crop_x[0]+28,crop_y[0]:crop_y[0]+28,:]\n",
    "            img = scipy.misc.imresize(img,(32,32))\n",
    "            \n",
    "        X[i,:,:,:] = img\n",
    "    return X\n",
    "    \n",
    "def get_batch(X, y, batch_size,list_,counter):\n",
    "    \"\"\"\n",
    "    Return minibatch of samples and labels\n",
    "\n",
    "    :param X, y: samples and corresponding labels\n",
    "    :parma batch_size: minibatch size\n",
    "    :returns: (tuple) X_batch, y_batch\n",
    "    \"\"\"\n",
    "    idx = list_[counter:counter+batch_size]\n",
    "    X_batch = X[idx,:,:,:]\n",
    "    y_batch = y[idx]\n",
    "    X_batch = augment(X_batch)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 7, 'automobile': 1, 'deer': 4, 'dog': 5, 'frog': 6, 'cat': 3, 'truck': 9, 'ship': 8, 'airplane': 0, 'bird': 2}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input normalization\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "X_train_stddev = np.std(X_train,axis=0)\n",
    "X_train = (X_train-X_train_mean)/X_train_stddev\n",
    "X_test = (X_test-X_train_mean)/X_train_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iters_max=20000\n",
    "batch_size=100\n",
    "split_ratio = 0.9\n",
    "lr_ = 0.1\n",
    "orig_full = range(len(y_train))\n",
    "np.random.shuffle(orig_full)\n",
    "split_idx = int((len(orig_full)*split_ratio))\n",
    "orig_train = orig_full[:split_idx]\n",
    "orig_val = orig_full[split_idx:]\n",
    "counter = 0\n",
    "train_idx = []\n",
    "epocs = (iters_max*batch_size)/len(orig_train) + 1\n",
    "for i in range(epocs):\n",
    "    np.random.shuffle(orig_train)\n",
    "    train_idx.extend(orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conv1\n",
    "images = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "gt = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "lr = tf.placeholder(tf.float32, shape=())\n",
    "drop_prob = tf.placeholder(tf.float32)\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    init = tf.truncated_normal_initializer(stddev=1e-2, dtype=tf.float32)\n",
    "    kernel = tf.get_variable('weights', shape=[5, 5, 3, 64], initializer=init, dtype=tf.float32)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    \n",
    "# pool1\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                     padding='SAME', name='pool1')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1') \n",
    "# conv2\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    init = tf.truncated_normal_initializer(stddev=1e-2, dtype=tf.float32)\n",
    "    kernel = tf.get_variable('weights', shape=[5, 5, 64, 64], initializer=init, dtype=tf.float32)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    \n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                   padding='SAME', name='pool2')\n",
    "norm2 = tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2') \n",
    " \n",
    "\n",
    "\n",
    "with tf.variable_scope('FC1') as scope:\n",
    "    reshape = tf.reshape(norm2, [batch_size, -1])\n",
    "    init = tf.truncated_normal_initializer(stddev=1/4096.0, dtype=tf.float32)\n",
    "    weights = tf.get_variable('weights', shape=[4096,512], initializer=init, dtype=tf.float32)\n",
    "    biases = tf.get_variable('biases', shape=[512], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    fc1 = tf.add(tf.matmul(reshape, weights), biases, name=scope.name)\n",
    "    fc1 = tf.nn.relu(fc1, name=scope.name)\n",
    "    \n",
    "#Fully connected layer\n",
    "\n",
    "fc1_drop = tf.nn.dropout(fc1, drop_prob)\n",
    "\n",
    "with tf.variable_scope('FC2') as scope:\n",
    "    reshape = tf.reshape(fc1_drop, [batch_size, -1])\n",
    "    init = tf.truncated_normal_initializer(stddev=1/512.0, dtype=tf.float32)\n",
    "    weights = tf.get_variable('weights', shape=[512,10], initializer=init, dtype=tf.float32)\n",
    "    biases = tf.get_variable('biases', shape=[10], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    before_softmax = tf.add(tf.matmul(reshape, weights), biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = before_softmax, labels = gt)\n",
    "loss = tf.reduce_mean(loss)\n",
    "correct_prediction_df = tf.equal(tf.argmax(before_softmax,1), tf.argmax(gt,1))\n",
    "accuracy_df = tf.reduce_mean(tf.cast(correct_prediction_df, tf.float32))\n",
    "train_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('train_acc',accuracy_df)\n",
    "conv1_grad = tf.gradients(loss, [conv1])[0]\n",
    "conv2_grad = tf.gradients(loss, [conv2])[0]\n",
    "fc2_grad = tf.gradients(loss, [before_softmax])[0]\n",
    "fc1_grad = tf.gradients(loss, [fc1])[0]\n",
    "tf.summary.histogram('conv1_grad',conv1_grad)\n",
    "tf.summary.histogram('conv2_grad',conv2_grad)\n",
    "tf.summary.histogram('fc1_grad',fc1_grad)\n",
    "tf.summary.histogram('fc2_grad',fc2_grad)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('tensorboard/')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(lr,beta1=0.9,beta2=0.999,epsilon=1e-08,).minimize(loss)\n",
    "best_till_now = 0.0\n",
    "best_train = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.30249\n",
      "val accuracy  0.108 train accuracy  0.0991778\n",
      "100 2.23563\n",
      "200 2.17531\n",
      "300 2.11725\n",
      "400 1.77753\n",
      "500 1.84121\n",
      "val accuracy  0.3578 train accuracy  0.364556\n",
      "600 1.66612\n",
      "700 1.65747\n",
      "800 1.48225\n",
      "900 1.60348\n",
      "1000 1.49022\n",
      "val accuracy  0.5054 train accuracy  0.505022\n",
      "1100 1.39816\n",
      "1200 1.42726\n",
      "1300 1.29948\n",
      "1400 1.27413\n",
      "1500 1.36545\n",
      "val accuracy  0.5174 train accuracy  0.532044\n",
      "1600 1.11434\n",
      "1700 1.1159\n",
      "1800 0.98458\n",
      "1900 1.04805\n",
      "2000 1.27567\n",
      "val accuracy  0.6198 train accuracy  0.633689\n",
      "2100 1.01284\n",
      "2200 0.917304\n",
      "2300 1.07408\n",
      "2400 1.06055\n",
      "2500 1.07198\n",
      "val accuracy  0.654 train accuracy  0.660444\n",
      "2600 1.18849\n",
      "2700 1.08467\n",
      "2800 0.996312\n",
      "2900 1.07904\n",
      "3000 1.06626\n",
      "val accuracy  0.6462 train accuracy  0.665089\n",
      "3100 1.13309\n",
      "3200 0.991364\n",
      "3300 0.985834\n",
      "3400 0.912851\n",
      "3500 0.795559\n",
      "val accuracy  0.7038 train accuracy  0.726422\n",
      "3600 0.976513\n",
      "3700 1.06896\n",
      "3800 0.990118\n",
      "3900 0.933223\n",
      "4000 0.958119\n",
      "val accuracy  0.6618 train accuracy  0.682089\n",
      "4100 0.721821\n",
      "4200 0.867363\n",
      "4300 0.785517\n",
      "4400 0.963077\n",
      "4500 0.967471\n",
      "val accuracy  0.733 train accuracy  0.754578\n",
      "4600 1.01771\n",
      "4700 0.761531\n",
      "4800 0.778346\n",
      "4900 1.00649\n",
      "5000 0.653907\n",
      "val accuracy  0.722 train accuracy  0.751044\n",
      "5100 0.752616\n",
      "5200 0.656139\n",
      "5300 0.85061\n",
      "5400 0.705599\n",
      "5500 0.776143\n",
      "val accuracy  0.7366 train accuracy  0.773756\n",
      "5600 0.560344\n",
      "5700 0.821413\n",
      "5800 0.82434\n",
      "5900 0.88398\n",
      "6000 0.885594\n",
      "val accuracy  0.7232 train accuracy  0.763933\n",
      "6100 0.870743\n",
      "6200 0.772546\n",
      "6300 0.724876\n",
      "6400 0.911658\n",
      "6500 0.929427\n",
      "val accuracy  0.731 train accuracy  0.779667\n",
      "6600 0.700597\n",
      "6700 0.878824\n",
      "6800 0.85953\n",
      "6900 0.604058\n",
      "7000 0.811987\n",
      "val accuracy  0.7418 train accuracy  0.794422\n",
      "7100 0.701498\n",
      "7200 0.690093\n",
      "7300 0.646603\n",
      "7400 0.52585\n",
      "7500 0.749031\n",
      "val accuracy  0.7636 train accuracy  0.810711\n",
      "7600 0.886734\n",
      "7700 0.604274\n",
      "7800 0.664619\n",
      "7900 0.753426\n",
      "8000 0.756763\n",
      "val accuracy  0.7688 train accuracy  0.822267\n",
      "8100 0.499921\n",
      "8200 0.722811\n",
      "8300 0.56224\n",
      "8400 0.55835\n",
      "8500 0.678847\n",
      "val accuracy  0.7636 train accuracy  0.820556\n",
      "8600 0.71721\n",
      "8700 0.720015\n",
      "8800 0.520169\n",
      "8900 0.515363\n",
      "9000 0.445064\n",
      "val accuracy  0.7698 train accuracy  0.827133\n",
      "9100 0.722526\n",
      "9200 0.541876\n",
      "9300 0.461222\n",
      "9400 0.874145\n",
      "9500 0.568541\n",
      "val accuracy  0.7636 train accuracy  0.825467\n",
      "9600 0.714203\n",
      "9700 0.685556\n",
      "9800 0.588523\n",
      "9900 0.709737\n",
      "10000 0.488566\n",
      "val accuracy  0.764 train accuracy  0.835822\n",
      "10100 0.425115\n",
      "10200 0.55753\n",
      "10300 0.578326\n",
      "10400 0.396217\n",
      "10500 0.49352\n",
      "val accuracy  0.789 train accuracy  0.863978\n",
      "10600 0.420529\n",
      "10700 0.46782\n",
      "10800 0.529151\n",
      "10900 0.514193\n",
      "11000 0.511177\n",
      "val accuracy  0.7864 train accuracy  0.867711\n",
      "11100 0.350664\n",
      "11200 0.514114\n",
      "11300 0.550708\n",
      "11400 0.400528\n",
      "11500 0.430725\n",
      "val accuracy  0.7972 train accuracy  0.872844\n",
      "11600 0.426672\n",
      "11700 0.528057\n",
      "11800 0.426687\n",
      "11900 0.657737\n",
      "12000 0.386073\n",
      "val accuracy  0.7894 train accuracy  0.871133\n",
      "12100 0.346099\n",
      "12200 0.584514\n",
      "12300 0.448833\n",
      "12400 0.344864\n",
      "12500 0.461836\n",
      "val accuracy  0.7966 train accuracy  0.877044\n",
      "12600 0.446631\n",
      "12700 0.542972\n",
      "12800 0.445371\n",
      "12900 0.470613\n",
      "13000 0.630882\n",
      "val accuracy  0.7986 train accuracy  0.877378\n",
      "13100 0.56238\n",
      "13200 0.508236\n",
      "13300 0.537454\n",
      "13400 0.3183\n",
      "13500 0.468823\n",
      "val accuracy  0.794 train accuracy  0.879111\n",
      "13600 0.395115\n",
      "13700 0.386932\n",
      "13800 0.501379\n",
      "13900 0.464294\n",
      "14000 0.453053\n",
      "val accuracy  0.793 train accuracy  0.8812\n",
      "14100 0.457036\n",
      "14200 0.39643\n",
      "14300 0.335477\n",
      "14400 0.390383\n",
      "14500 0.422701\n",
      "val accuracy  0.8048 train accuracy  0.881156\n",
      "14600 0.294673\n",
      "14700 0.516645\n",
      "14800 0.474582\n",
      "14900 0.389816\n",
      "15000 0.373976\n",
      "val accuracy  0.7968 train accuracy  0.883333\n",
      "15100 0.641402\n",
      "15200 0.422627\n",
      "15300 0.364476\n",
      "15400 0.400006\n",
      "15500 0.389278\n",
      "val accuracy  0.805 train accuracy  0.884267\n",
      "15600 0.522795\n",
      "15700 0.49145\n",
      "15800 0.416105\n",
      "15900 0.413632\n",
      "16000 0.401796\n",
      "val accuracy  0.804 train accuracy  0.886022\n",
      "16100 0.351097\n",
      "16200 0.44449\n",
      "16300 0.329656\n",
      "16400 0.549912\n",
      "16500 0.347378\n",
      "val accuracy  0.805 train accuracy  0.885933\n",
      "16600 0.369979\n",
      "16700 0.385627\n",
      "16800 0.462878\n",
      "16900 0.462321\n",
      "17000 0.421835\n",
      "val accuracy  0.806 train accuracy  0.890622\n",
      "17100 0.363593\n",
      "17200 0.442323\n",
      "17300 0.37248\n",
      "17400 0.45647\n",
      "17500 0.372414\n",
      "val accuracy  0.8044 train accuracy  0.889222\n",
      "17600 0.625104\n",
      "17700 0.463171\n",
      "17800 0.507239\n",
      "17900 0.36821\n",
      "18000 0.521897\n",
      "val accuracy  0.8002 train accuracy  0.8922\n",
      "18100 0.332343\n",
      "18200 0.364443\n",
      "18300 0.563815\n",
      "18400 0.330156\n",
      "18500 0.57578\n",
      "val accuracy  0.8012 train accuracy  0.890644\n",
      "18600 0.535867\n",
      "18700 0.386016\n",
      "18800 0.425685\n",
      "18900 0.407092\n",
      "19000 0.336974\n",
      "val accuracy  0.8048 train accuracy  0.893311\n",
      "19100 0.323859\n",
      "19200 0.416663\n",
      "19300 0.260285\n",
      "19400 0.455376\n",
      "19500 0.467937\n",
      "val accuracy  0.8116 train accuracy  0.895978\n",
      "19600 0.445269\n",
      "19700 0.382669\n",
      "19800 0.421879\n",
      "19900 0.334528\n"
     ]
    }
   ],
   "source": [
    "with  tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for iter in range(iters_max):\n",
    "        X_batch, y_batch = get_batch( X_train, y_train, batch_size,train_idx,counter)\n",
    "        counter += batch_size\n",
    "        y_batch = one_hot(y_batch).transpose((1,0))\n",
    "        #loss.run(feed_dict={images: X_batch, gt: y_batch})\n",
    "        if iter%10000 == 1:\n",
    "            lr_/=10.0\n",
    "        _, loss_npy,summary = sess.run([train_step,loss,merged],feed_dict={images: X_batch, gt: y_batch,lr:lr_,drop_prob: 0.5})\n",
    "        \n",
    "\n",
    "        if iter%100==0:\n",
    "            print iter, loss_npy\n",
    "        if iter%500==0:\n",
    "            test_counter = 0\n",
    "            train_counter = 0\n",
    "            acc_val  = []\n",
    "            acc_train  = []\n",
    "            while test_counter < len(orig_val):  # check if we need to add +1 or -1 REMOVE this BUG\n",
    "                X_batch, y_batch = get_batch( X_train, y_train, batch_size,orig_val,test_counter)\n",
    "                y_batch = one_hot(y_batch).transpose((1,0))\n",
    "                test_counter+=batch_size\n",
    "                acc_val.append(sess.run([accuracy_df],feed_dict={images: X_batch, gt: y_batch,lr:lr_, drop_prob: 1.0}))\n",
    "            while train_counter < len(orig_train):  # check if we need to add +1 or -1 REMOVE this BUG\n",
    "                X_batch, y_batch = get_batch( X_train, y_train, batch_size,orig_train,train_counter)\n",
    "                y_batch = one_hot(y_batch).transpose((1,0))\n",
    "                train_counter+=batch_size\n",
    "                acc_train.append(sess.run([accuracy_df],feed_dict={images: X_batch, gt: y_batch, lr:lr_, drop_prob: 1.0}))\n",
    "            print 'val accuracy ', np.mean(np.asarray(acc_val)),'train accuracy ', np.mean(np.asarray(acc_train))\n",
    "            if np.mean(np.asarray(acc_val)) > best_till_now:\n",
    "                best_till_now = np.mean(np.asarray(acc_val))\n",
    "                best_train = np.mean(np.asarray(acc_train))\n",
    "                saver.save(sess, 'part2/best-val-model-part2')\n",
    "            train_acc = np.mean(np.asarray(acc_train))\n",
    "        train_writer.add_summary(summary, iter)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val accuracy -  0.8116 best train accuracy 0.895978\n"
     ]
    }
   ],
   "source": [
    "print 'best val accuracy - ',best_till_now, 'best train accuracy',best_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.import_meta_graph('part2/best-val-model-part2.meta')\n",
    "    saver.restore(sess, 'part2/best-val-model-part2')\n",
    "    test_counter = 0\n",
    "    y_pred = np.zeros((10,X_test.shape[0]))\n",
    "    count = 0\n",
    "    test_order_list = range(X_test.shape[0])\n",
    "    y_test = np.zeros((X_test.shape[0]),dtype=np.int) #fake y_test values, will not be used\n",
    "    while test_counter < len(test_order_list): \n",
    "        X_batch, y_batch = get_batch( X_test, y_test, batch_size,test_order_list,test_counter)\n",
    "        y_batch = one_hot(y_batch).transpose((1,0))\n",
    "        test_counter+=batch_size\n",
    "        batch_scores = sess.run([before_softmax],feed_dict={images: X_batch, gt: y_batch,lr:lr_, drop_prob: 1.0})\n",
    "        y_pred[:,count:count+batch_size] = batch_scores[0].transpose((1,0))\n",
    "        count+=batch_size\n",
    "\n",
    "    np.save('part2/ans2-uni.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 6274.23906803\n"
     ]
    }
   ],
   "source": [
    "t2 = time()\n",
    "print 'time taken', t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
