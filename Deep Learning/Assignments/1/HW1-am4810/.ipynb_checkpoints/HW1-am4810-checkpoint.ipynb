{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 1\n",
    "Due on Monday, Oct 9, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 3 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the UNIs of your group (if applicable)\n",
    "\n",
    "Member 1: Isht Dwivedi, id2303\n",
    "\n",
    "Member 2: Abhinav Sharma, as5414\n",
    "\n",
    "Member 3: Aayush Maini, am4810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of our Net\n",
    "\n",
    "### Common parameters for all parts\n",
    "2 hidden layers, first with 250 nodes, second with 200 nodes\n",
    "\n",
    "batch size is 500\n",
    "\n",
    "train val split is 9:1. \n",
    "\n",
    "### For part 1:\n",
    "\n",
    "Constant learning rate of 0.0001 for 10000 iterations\n",
    "\n",
    "#### Results for part 1: validation accuracy 55.05% , training accuracy 66.70%\n",
    " \n",
    "### For part 2:\n",
    "\n",
    "Step decay of learning rate. Initial value of 0.0001 for 15000 iterations, after which it is divided by 10\n",
    "\n",
    "Dropout rate of 0.2 used. \n",
    "\n",
    "Regularization parameter value taken as 0.1 \n",
    "\n",
    "#### Results for part 2: validation accuracy 57.79%, training accuracy 71.95%\n",
    "\n",
    "\n",
    "### Note: optional part in a seperate file named \"HW1-optional-part.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import copy\n",
    "from time import time\n",
    "t1 = time()\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        self.num_classes = 10\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        #self.batch_size = \n",
    "        self.parameters = {} # layer->[W,b]\n",
    "        self.best_parameters = {}\n",
    "        # Xavier init : w ~ N(0,1/sqrt(fan_in)); b = 0.01\n",
    "        for i in range(1,self.num_layers):\n",
    "            l_cur = layer_dimensions[i]\n",
    "            l_prev = layer_dimensions[i-1] \n",
    "            bias = 0.01*np.ones((l_cur,1))\n",
    "            std_dev = 1/np.sqrt(l_prev)\n",
    "            self.parameters[i] = [np.random.normal(0,std_dev,(l_cur,l_prev)),bias]\n",
    "        \n",
    "        for i in self.parameters.keys():\n",
    "            self.best_parameters[i] = copy.deepcopy(self.parameters[i])\n",
    "        #self.drop_prob = \n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.drop_prob = 1.0 - drop_prob\n",
    "        # init parameters\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        y = np.matmul(W,A) + b\n",
    "        cache = [A,y,W]\n",
    "        return (y,cache)\n",
    "        \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        Act = self.relu(A)\n",
    "        return Act\n",
    "\n",
    "    def relu(self, X):\n",
    "        A = np.maximum(0,X)\n",
    "        return A\n",
    "        \n",
    "\n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: \n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = (np.random.rand(*A.shape)<prob)/prob\n",
    "        A = A*M\n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        A_prev = X\n",
    "        S = X.shape[1]\n",
    "        cache = {} #layer->(A,y,W,mask)\n",
    "        for i in range(1,self.num_layers):\n",
    "            num_nodes = self.layer_dimensions[i]\n",
    "            A_cur = np.zeros((num_nodes,S))\n",
    "            A_cur,cache[i] = self.affineForward(A_prev, self.parameters[i][0], self.parameters[i][1])\n",
    "            A_prev = self.activationForward(A_cur)\n",
    "            if self.drop_prob > 0:\n",
    "                A_prev,temp = self.dropout(A_prev,self.drop_prob)\n",
    "                cache[i].append(temp)\n",
    "        AL = A_cur\n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        cost = 0\n",
    "        S = len(AL[0])\n",
    "        dAL = np.zeros((self.num_classes,S))\n",
    "        for i in range(S):\n",
    "            indx = np.argmax(y[:,i])\n",
    "            exp = np.exp(AL[:,i])\n",
    "            y_hat = exp/np.sum(exp)\n",
    "            val = exp[indx]/np.sum(exp)\n",
    "            cost += -np.log(val)\n",
    "            dAL[:,i] = y_hat - y[:,i]\n",
    "        cost /= S\n",
    "\n",
    "        \n",
    "        if self.reg_lambda > 0:\n",
    "            for i in range(1,self.num_layers):\n",
    "                cost+= 0.5 * self.reg_lambda * np.sum(self.parameters[i][0]**2)\n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "\n",
    "        dA = np.matmul(cache[2].transpose(),dA_prev)\n",
    "        dW = np.matmul(dA_prev,cache[0].transpose())#/len(cache[0]) #divide here or \n",
    "        db = np.sum(dA_prev, axis=1)[np.newaxis,:] # to do\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        dx = self.relu_derivative(dA,cache)\n",
    "        return dx\n",
    "    \n",
    "    def relu_derivative(self, dx, cache):\n",
    "        dx_relu = dx\n",
    "        dx_relu[cache[1] <= 0] = 0\n",
    "        return dx_relu\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "        dA = cache[3]*dA\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {} #layer->(dw,db)\n",
    "        dA_prev = dAL\n",
    "        for i in range(self.num_layers-1,0,-1): # changed to 0, as we dont need to backprop the first layer\n",
    "            if self.drop_prob > 0:\n",
    "                if i<self.num_layers-1:\n",
    "                    dA_prev = self.dropout_backward(dA_prev,cache[i])\n",
    "            dA, dW, db = self.affineBackward(dA_prev, cache[i])\n",
    "            if i==1:\n",
    "                # No relu after input layer\n",
    "                dx = dA\n",
    "            else:\n",
    "                dx = self.activationBackward(dA,cache[i-1])\n",
    "            dA_prev = dx\n",
    "            gradients[i] = [dW,db]\n",
    "           \n",
    "            \n",
    "        if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "            for i in range(self.num_layers-1,0,-1): \n",
    "                gradients[i][0]+=self.reg_lambda * self.parameters[i][0]\n",
    "            \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        for i in range(1,self.num_layers):\n",
    "            self.parameters[i][0] -= alpha*gradients[i][0]\n",
    "            self.parameters[i][1] -= alpha*gradients[i][1].transpose()\n",
    "        return\n",
    "    \n",
    "    def predict(self, X,mode ='train'):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        if mode is 'test':\n",
    "            print 'predict mode is test'\n",
    "            for ke in self.best_parameters.keys():\n",
    "                self.parameters[ke] = copy.deepcopy(self.best_parameters[ke])\n",
    "        y_pred = np.zeros(X.shape[1],dtype=np.int)\n",
    "        y_pred_prob = np.zeros((10,X.shape[1]),dtype=np.float)\n",
    "        temp = self.drop_prob\n",
    "        self.drop_prob = 0.0\n",
    "        for i in range(X.shape[1]):\n",
    "            AL = self.forwardPropagation(X[:,i][np.newaxis,:].transpose())[0]\n",
    "            y_pred_prob[:,i] = AL[:,0]\n",
    "            y_pred[i] = np.argmax(AL[:,0])\n",
    "        self.drop_prob = temp\n",
    "        return y_pred,y_pred_prob\n",
    "\n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        best_val_till_now = 0 \n",
    "        train_idx = []\n",
    "        split_ratio = 0.9\n",
    "        orig_full = range(len(y_train))\n",
    "        np.random.shuffle(orig_full)\n",
    "        split_idx = int((len(orig_full)*split_ratio))\n",
    "        orig_train = orig_full[:split_idx]\n",
    "        orig_val = orig_full[split_idx+1:]\n",
    "        counter = 0\n",
    "        epocs = (iters*batch_size)/len(orig_train) + 1\n",
    "        for i in range(epocs):\n",
    "            np.random.shuffle(orig_train)\n",
    "            train_idx.extend(orig_train)\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            if (i+2)%15000==0:\n",
    "                alpha = alpha/10.0\n",
    "            X_batch, y_batch = get_batch(self, X, y, batch_size,train_idx,counter)\n",
    "            counter += batch_size\n",
    "            y_batch = one_hot(y_batch)\n",
    "            AL, cache = self.forwardPropagation(X_batch)\n",
    "            # forward prop\n",
    "            cost, dAL = self.costFunction(AL, y_batch)\n",
    "            # compute loss\n",
    "            gradients = self.backPropagation( dAL, y, cache)\n",
    "            # compute gradients\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            # update weights and biases based on gradient\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                # Compute training accuracy\n",
    "                X_t = X[:,orig_train]\n",
    "                y_pred_t = self.predict(X_t,'train')[0]\n",
    "                y_t = y[orig_train]\n",
    "                tx_acc = 1 - float(np.count_nonzero(y_pred_t - y_t))/len(orig_train)\n",
    "                # Compute validation accuracy\n",
    "                X_v = X[:,orig_val]\n",
    "                y_pred_v = self.predict(X_v,'train')[0]\n",
    "                y_v = y[orig_val]\n",
    "                tv_acc = 1 - float(np.count_nonzero(y_pred_v - y_v))/len(orig_val)\n",
    "                \n",
    "                if tv_acc>best_val_till_now:\n",
    "                    best_val_till_now = tv_acc\n",
    "                    train_acc_till_now = tx_acc\n",
    "                    for ke in self.parameters.keys():\n",
    "                        self.best_parameters[ke] = copy.deepcopy(self.parameters[ke])\n",
    "                print 'Iter: ',i,'loss: ',cost,'val_accuracy: ',tv_acc,'train_accuracy: ',tx_acc\n",
    "        print 'best val accuracy: ',best_val_till_now,'train accuacy at this point: ',train_acc_till_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_batch(self, X, y, batch_size,train_idx,counter):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "\n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        idx = train_idx[counter:counter+batch_size]\n",
    "        X_batch = X[:,idx]\n",
    "        y_batch = y[idx]\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_one_hot[i,y[i]]=1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 7, 'automobile': 1, 'deer': 4, 'dog': 5, 'frog': 6, 'cat': 3, 'truck': 9, 'ship': 8, 'airplane': 0, 'bird': 2}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0 loss:  2.32632854468 val_accuracy:  0.143228645729 train_accuracy:  0.136888888889\n",
      "Iter:  50 loss:  1.99885255311 val_accuracy:  0.316063212643 train_accuracy:  0.311222222222\n",
      "Iter:  100 loss:  1.98324125513 val_accuracy:  0.304460892178 train_accuracy:  0.295066666667\n",
      "Iter:  150 loss:  1.90736132054 val_accuracy:  0.325665133027 train_accuracy:  0.326577777778\n",
      "Iter:  200 loss:  1.98137646225 val_accuracy:  0.313062612523 train_accuracy:  0.311333333333\n",
      "Iter:  250 loss:  1.81494481428 val_accuracy:  0.353270654131 train_accuracy:  0.356755555556\n",
      "Iter:  300 loss:  1.76172608178 val_accuracy:  0.371474294859 train_accuracy:  0.377688888889\n",
      "Iter:  350 loss:  1.74387287098 val_accuracy:  0.358471694339 train_accuracy:  0.362422222222\n",
      "Iter:  400 loss:  1.74867775335 val_accuracy:  0.344068813763 train_accuracy:  0.349355555556\n",
      "Iter:  450 loss:  1.66949562584 val_accuracy:  0.383076615323 train_accuracy:  0.384733333333\n",
      "Iter:  500 loss:  1.69095670865 val_accuracy:  0.367673534707 train_accuracy:  0.364088888889\n",
      "Iter:  550 loss:  1.62895692154 val_accuracy:  0.370474094819 train_accuracy:  0.378844444444\n",
      "Iter:  600 loss:  1.66869383522 val_accuracy:  0.407281456291 train_accuracy:  0.409511111111\n",
      "Iter:  650 loss:  1.58254485408 val_accuracy:  0.401480296059 train_accuracy:  0.409066666667\n",
      "Iter:  700 loss:  1.62701463083 val_accuracy:  0.406881376275 train_accuracy:  0.411511111111\n",
      "Iter:  750 loss:  1.75134823206 val_accuracy:  0.396679335867 train_accuracy:  0.400222222222\n",
      "Iter:  800 loss:  1.53056589648 val_accuracy:  0.417683536707 train_accuracy:  0.4134\n",
      "Iter:  850 loss:  1.62181212889 val_accuracy:  0.402880576115 train_accuracy:  0.408333333333\n",
      "Iter:  900 loss:  1.61393776223 val_accuracy:  0.409481896379 train_accuracy:  0.417711111111\n",
      "Iter:  950 loss:  1.67529380794 val_accuracy:  0.420284056811 train_accuracy:  0.431711111111\n",
      "Iter:  1000 loss:  1.53401070936 val_accuracy:  0.435887177435 train_accuracy:  0.443688888889\n",
      "Iter:  1050 loss:  1.61596105273 val_accuracy:  0.448289657932 train_accuracy:  0.459644444444\n",
      "Iter:  1100 loss:  1.58390884647 val_accuracy:  0.397279455891 train_accuracy:  0.400644444444\n",
      "Iter:  1150 loss:  1.5585611584 val_accuracy:  0.451890378076 train_accuracy:  0.4658\n",
      "Iter:  1200 loss:  1.60118857607 val_accuracy:  0.444288857772 train_accuracy:  0.4568\n",
      "Iter:  1250 loss:  1.63943844502 val_accuracy:  0.434486897379 train_accuracy:  0.4354\n",
      "Iter:  1300 loss:  1.49954198847 val_accuracy:  0.453690738148 train_accuracy:  0.472888888889\n",
      "Iter:  1350 loss:  1.64845769929 val_accuracy:  0.433886777355 train_accuracy:  0.442266666667\n",
      "Iter:  1400 loss:  1.44831921939 val_accuracy:  0.460692138428 train_accuracy:  0.472666666667\n",
      "Iter:  1450 loss:  1.49589989996 val_accuracy:  0.456891378276 train_accuracy:  0.473577777778\n",
      "Iter:  1500 loss:  1.537853054 val_accuracy:  0.451890378076 train_accuracy:  0.470133333333\n",
      "Iter:  1550 loss:  1.45294705449 val_accuracy:  0.451090218044 train_accuracy:  0.4592\n",
      "Iter:  1600 loss:  1.45323966792 val_accuracy:  0.428885777155 train_accuracy:  0.435711111111\n",
      "Iter:  1650 loss:  1.393937072 val_accuracy:  0.4674934987 train_accuracy:  0.489177777778\n",
      "Iter:  1700 loss:  1.48552215972 val_accuracy:  0.47349469894 train_accuracy:  0.488755555556\n",
      "Iter:  1750 loss:  1.52513057132 val_accuracy:  0.454890978196 train_accuracy:  0.463844444444\n",
      "Iter:  1800 loss:  1.44423124347 val_accuracy:  0.474894978996 train_accuracy:  0.4894\n",
      "Iter:  1850 loss:  1.52081003209 val_accuracy:  0.463292658532 train_accuracy:  0.476177777778\n",
      "Iter:  1900 loss:  1.42000894053 val_accuracy:  0.45349069814 train_accuracy:  0.467066666667\n",
      "Iter:  1950 loss:  1.54418780641 val_accuracy:  0.469293858772 train_accuracy:  0.488977777778\n",
      "Iter:  2000 loss:  1.54228070523 val_accuracy:  0.45949189838 train_accuracy:  0.471266666667\n",
      "Iter:  2050 loss:  1.46178182975 val_accuracy:  0.472094418884 train_accuracy:  0.485177777778\n",
      "Iter:  2100 loss:  1.45358682351 val_accuracy:  0.45849169834 train_accuracy:  0.485\n",
      "Iter:  2150 loss:  1.45081367106 val_accuracy:  0.441288257652 train_accuracy:  0.450688888889\n",
      "Iter:  2200 loss:  1.58005364332 val_accuracy:  0.461292258452 train_accuracy:  0.473555555556\n",
      "Iter:  2250 loss:  1.34576947811 val_accuracy:  0.487697539508 train_accuracy:  0.5088\n",
      "Iter:  2300 loss:  1.38111332645 val_accuracy:  0.48949789958 train_accuracy:  0.509777777778\n",
      "Iter:  2350 loss:  1.35957475198 val_accuracy:  0.496899379876 train_accuracy:  0.521044444444\n",
      "Iter:  2400 loss:  1.47662822427 val_accuracy:  0.48849769954 train_accuracy:  0.508444444444\n",
      "Iter:  2450 loss:  1.46018007584 val_accuracy:  0.459091818364 train_accuracy:  0.477866666667\n",
      "Iter:  2500 loss:  1.44801479889 val_accuracy:  0.46449289858 train_accuracy:  0.493155555556\n",
      "Iter:  2550 loss:  1.38853399889 val_accuracy:  0.492298459692 train_accuracy:  0.520577777778\n",
      "Iter:  2600 loss:  1.37213623463 val_accuracy:  0.493898779756 train_accuracy:  0.519044444444\n",
      "Iter:  2650 loss:  1.51507334096 val_accuracy:  0.48549709942 train_accuracy:  0.509088888889\n",
      "Iter:  2700 loss:  1.31046299957 val_accuracy:  0.497099419884 train_accuracy:  0.5284\n",
      "Iter:  2750 loss:  1.38551210328 val_accuracy:  0.494098819764 train_accuracy:  0.520466666667\n",
      "Iter:  2800 loss:  1.40270051437 val_accuracy:  0.492298459692 train_accuracy:  0.520022222222\n",
      "Iter:  2850 loss:  1.32261847295 val_accuracy:  0.48649729946 train_accuracy:  0.517533333333\n",
      "Iter:  2900 loss:  1.33595415616 val_accuracy:  0.501300260052 train_accuracy:  0.531088888889\n",
      "Iter:  2950 loss:  1.33242161198 val_accuracy:  0.487297459492 train_accuracy:  0.525911111111\n",
      "Iter:  3000 loss:  1.30329702905 val_accuracy:  0.49049809962 train_accuracy:  0.515777777778\n",
      "Iter:  3050 loss:  1.3638985405 val_accuracy:  0.508101620324 train_accuracy:  0.537155555556\n",
      "Iter:  3100 loss:  1.33566876624 val_accuracy:  0.484896979396 train_accuracy:  0.509044444444\n",
      "Iter:  3150 loss:  1.3222407556 val_accuracy:  0.4924984997 train_accuracy:  0.535422222222\n",
      "Iter:  3200 loss:  1.31388281989 val_accuracy:  0.49049809962 train_accuracy:  0.532777777778\n",
      "Iter:  3250 loss:  1.38408267963 val_accuracy:  0.497699539908 train_accuracy:  0.525577777778\n",
      "Iter:  3300 loss:  1.36452610051 val_accuracy:  0.48549709942 train_accuracy:  0.525244444444\n",
      "Iter:  3350 loss:  1.39224841629 val_accuracy:  0.500700140028 train_accuracy:  0.538688888889\n",
      "Iter:  3400 loss:  1.46888310685 val_accuracy:  0.510102020404 train_accuracy:  0.538422222222\n",
      "Iter:  3450 loss:  1.34541958239 val_accuracy:  0.5025005001 train_accuracy:  0.540622222222\n",
      "Iter:  3500 loss:  1.43524352953 val_accuracy:  0.48549709942 train_accuracy:  0.520088888889\n",
      "Iter:  3550 loss:  1.35911076101 val_accuracy:  0.489297859572 train_accuracy:  0.523422222222\n",
      "Iter:  3600 loss:  1.26129684215 val_accuracy:  0.49349869974 train_accuracy:  0.529822222222\n",
      "Iter:  3650 loss:  1.33983757899 val_accuracy:  0.487697539508 train_accuracy:  0.535022222222\n",
      "Iter:  3700 loss:  1.30785925781 val_accuracy:  0.487297459492 train_accuracy:  0.520777777778\n",
      "Iter:  3750 loss:  1.25110588414 val_accuracy:  0.504700940188 train_accuracy:  0.539666666667\n",
      "Iter:  3800 loss:  1.28845821709 val_accuracy:  0.515903180636 train_accuracy:  0.560977777778\n",
      "Iter:  3850 loss:  1.34030025631 val_accuracy:  0.510102020404 train_accuracy:  0.5556\n",
      "Iter:  3900 loss:  1.27820196675 val_accuracy:  0.50950190038 train_accuracy:  0.556155555556\n",
      "Iter:  3950 loss:  1.3457917248 val_accuracy:  0.503300660132 train_accuracy:  0.545466666667\n",
      "Iter:  4000 loss:  1.23952942256 val_accuracy:  0.509301860372 train_accuracy:  0.558244444444\n",
      "Iter:  4050 loss:  1.2327859311 val_accuracy:  0.512702540508 train_accuracy:  0.560422222222\n",
      "Iter:  4100 loss:  1.42198154446 val_accuracy:  0.505101020204 train_accuracy:  0.545466666667\n",
      "Iter:  4150 loss:  1.2269685096 val_accuracy:  0.504100820164 train_accuracy:  0.549733333333\n",
      "Iter:  4200 loss:  1.29050538083 val_accuracy:  0.505901180236 train_accuracy:  0.555066666667\n",
      "Iter:  4250 loss:  1.19574925647 val_accuracy:  0.51350270054 train_accuracy:  0.560022222222\n",
      "Iter:  4300 loss:  1.20858434297 val_accuracy:  0.507301460292 train_accuracy:  0.554666666667\n",
      "Iter:  4350 loss:  1.22765000564 val_accuracy:  0.519703940788 train_accuracy:  0.572511111111\n",
      "Iter:  4400 loss:  1.2830048106 val_accuracy:  0.508701740348 train_accuracy:  0.546488888889\n",
      "Iter:  4450 loss:  1.27885352729 val_accuracy:  0.518703740748 train_accuracy:  0.571133333333\n",
      "Iter:  4500 loss:  1.2730561272 val_accuracy:  0.518303660732 train_accuracy:  0.570088888889\n",
      "Iter:  4550 loss:  1.33958023294 val_accuracy:  0.511302260452 train_accuracy:  0.559177777778\n",
      "Iter:  4600 loss:  1.23481318817 val_accuracy:  0.457891578316 train_accuracy:  0.508888888889\n",
      "Iter:  4650 loss:  1.23274236086 val_accuracy:  0.521304260852 train_accuracy:  0.574044444444\n",
      "Iter:  4700 loss:  1.25381246483 val_accuracy:  0.527105421084 train_accuracy:  0.580511111111\n",
      "Iter:  4750 loss:  1.29443996022 val_accuracy:  0.50050010002 train_accuracy:  0.560555555556\n",
      "Iter:  4800 loss:  1.23134323435 val_accuracy:  0.524704940988 train_accuracy:  0.5812\n",
      "Iter:  4850 loss:  1.23629873922 val_accuracy:  0.511902380476 train_accuracy:  0.574266666667\n",
      "Iter:  4900 loss:  1.23505529395 val_accuracy:  0.513302660532 train_accuracy:  0.565955555556\n",
      "Iter:  4950 loss:  1.13267959315 val_accuracy:  0.520304060812 train_accuracy:  0.574355555556\n",
      "Iter:  5000 loss:  1.3229588174 val_accuracy:  0.5225045009 train_accuracy:  0.582533333333\n",
      "Iter:  5050 loss:  1.20248855385 val_accuracy:  0.509101820364 train_accuracy:  0.569755555556\n",
      "Iter:  5100 loss:  1.39670192358 val_accuracy:  0.492898579716 train_accuracy:  0.535\n",
      "Iter:  5150 loss:  1.11984625423 val_accuracy:  0.521104220844 train_accuracy:  0.583933333333\n",
      "Iter:  5200 loss:  1.32586868992 val_accuracy:  0.499899979996 train_accuracy:  0.557422222222\n",
      "Iter:  5250 loss:  1.27031275116 val_accuracy:  0.49549909982 train_accuracy:  0.563844444444\n",
      "Iter:  5300 loss:  1.13762741808 val_accuracy:  0.516303260652 train_accuracy:  0.577244444444\n",
      "Iter:  5350 loss:  1.23040523103 val_accuracy:  0.517903580716 train_accuracy:  0.586533333333\n",
      "Iter:  5400 loss:  1.28153531886 val_accuracy:  0.504700940188 train_accuracy:  0.550444444444\n",
      "Iter:  5450 loss:  1.25326195674 val_accuracy:  0.50650130026 train_accuracy:  0.562355555556\n",
      "Iter:  5500 loss:  1.15274313311 val_accuracy:  0.530306061212 train_accuracy:  0.591466666667\n",
      "Iter:  5550 loss:  1.12152874717 val_accuracy:  0.522704540908 train_accuracy:  0.592466666667\n",
      "Iter:  5600 loss:  1.14763530351 val_accuracy:  0.530706141228 train_accuracy:  0.601177777778\n",
      "Iter:  5650 loss:  1.20344266381 val_accuracy:  0.52450490098 train_accuracy:  0.593622222222\n",
      "Iter:  5700 loss:  1.12733225246 val_accuracy:  0.527105421084 train_accuracy:  0.597888888889\n",
      "Iter:  5750 loss:  1.20213030388 val_accuracy:  0.520904180836 train_accuracy:  0.587666666667\n",
      "Iter:  5800 loss:  1.24356234154 val_accuracy:  0.513702740548 train_accuracy:  0.586511111111\n",
      "Iter:  5850 loss:  1.1146430158 val_accuracy:  0.525105021004 train_accuracy:  0.601577777778\n",
      "Iter:  5900 loss:  1.13740864268 val_accuracy:  0.532306461292 train_accuracy:  0.604066666667\n",
      "Iter:  5950 loss:  1.12323763153 val_accuracy:  0.528305661132 train_accuracy:  0.601266666667\n",
      "Iter:  6000 loss:  1.23219996673 val_accuracy:  0.52350470094 train_accuracy:  0.603022222222\n",
      "Iter:  6050 loss:  1.32896977477 val_accuracy:  0.493698739748 train_accuracy:  0.566155555556\n",
      "Iter:  6100 loss:  1.16676307947 val_accuracy:  0.504700940188 train_accuracy:  0.578222222222\n",
      "Iter:  6150 loss:  1.09095484255 val_accuracy:  0.520304060812 train_accuracy:  0.592266666667\n",
      "Iter:  6200 loss:  1.16553819056 val_accuracy:  0.504900980196 train_accuracy:  0.589\n",
      "Iter:  6250 loss:  1.20901833341 val_accuracy:  0.516903380676 train_accuracy:  0.594866666667\n",
      "Iter:  6300 loss:  1.12152615497 val_accuracy:  0.525105021004 train_accuracy:  0.602333333333\n",
      "Iter:  6350 loss:  1.25046877561 val_accuracy:  0.509901980396 train_accuracy:  0.5762\n",
      "Iter:  6400 loss:  1.24237114425 val_accuracy:  0.525905181036 train_accuracy:  0.611022222222\n",
      "Iter:  6450 loss:  1.1955303948 val_accuracy:  0.505901180236 train_accuracy:  0.586488888889\n",
      "Iter:  6500 loss:  1.14342385777 val_accuracy:  0.522104420884 train_accuracy:  0.601844444444\n",
      "Iter:  6550 loss:  1.14979821132 val_accuracy:  0.529305861172 train_accuracy:  0.614444444444\n",
      "Iter:  6600 loss:  1.09097363095 val_accuracy:  0.535107021404 train_accuracy:  0.626466666667\n",
      "Iter:  6650 loss:  1.13212165009 val_accuracy:  0.496299259852 train_accuracy:  0.578311111111\n",
      "Iter:  6700 loss:  1.13499485749 val_accuracy:  0.518103620724 train_accuracy:  0.6092\n",
      "Iter:  6750 loss:  1.00038537997 val_accuracy:  0.53350670134 train_accuracy:  0.633355555556\n",
      "Iter:  6800 loss:  1.21286256364 val_accuracy:  0.510902180436 train_accuracy:  0.597111111111\n",
      "Iter:  6850 loss:  1.05418891771 val_accuracy:  0.516703340668 train_accuracy:  0.616044444444\n",
      "Iter:  6900 loss:  1.04809126675 val_accuracy:  0.51450290058 train_accuracy:  0.595333333333\n",
      "Iter:  6950 loss:  1.07056676369 val_accuracy:  0.504900980196 train_accuracy:  0.589244444444\n",
      "Iter:  7000 loss:  1.02690759876 val_accuracy:  0.51550310062 train_accuracy:  0.598133333333\n",
      "Iter:  7050 loss:  1.17144741787 val_accuracy:  0.5075015003 train_accuracy:  0.594844444444\n",
      "Iter:  7100 loss:  1.14082601093 val_accuracy:  0.544708941788 train_accuracy:  0.637911111111\n",
      "Iter:  7150 loss:  1.08994937531 val_accuracy:  0.53350670134 train_accuracy:  0.624822222222\n",
      "Iter:  7200 loss:  0.955260981989 val_accuracy:  0.538707741548 train_accuracy:  0.642222222222\n",
      "Iter:  7250 loss:  1.09952532301 val_accuracy:  0.519703940788 train_accuracy:  0.611577777778\n",
      "Iter:  7300 loss:  1.16432073316 val_accuracy:  0.5175035007 train_accuracy:  0.598422222222\n",
      "Iter:  7350 loss:  1.10135279843 val_accuracy:  0.5175035007 train_accuracy:  0.609577777778\n",
      "Iter:  7400 loss:  1.09436874645 val_accuracy:  0.533306661332 train_accuracy:  0.6284\n",
      "Iter:  7450 loss:  1.0983181401 val_accuracy:  0.519303860772 train_accuracy:  0.606688888889\n",
      "Iter:  7500 loss:  1.07410293904 val_accuracy:  0.537107421484 train_accuracy:  0.638177777778\n",
      "Iter:  7550 loss:  1.12890779814 val_accuracy:  0.5075015003 train_accuracy:  0.584044444444\n",
      "Iter:  7600 loss:  1.04064662544 val_accuracy:  0.5325065013 train_accuracy:  0.631377777778\n",
      "Iter:  7650 loss:  1.09238689725 val_accuracy:  0.51950390078 train_accuracy:  0.619933333333\n",
      "Iter:  7700 loss:  1.11917825658 val_accuracy:  0.520704140828 train_accuracy:  0.623688888889\n",
      "Iter:  7750 loss:  1.10789793027 val_accuracy:  0.503100620124 train_accuracy:  0.6066\n",
      "Iter:  7800 loss:  1.01436477261 val_accuracy:  0.54650930186 train_accuracy:  0.659333333333\n",
      "Iter:  7850 loss:  1.11298645749 val_accuracy:  0.51850370074 train_accuracy:  0.6288\n",
      "Iter:  7900 loss:  1.06262635384 val_accuracy:  0.5375075015 train_accuracy:  0.649711111111\n",
      "Iter:  7950 loss:  0.990108554554 val_accuracy:  0.520704140828 train_accuracy:  0.627311111111\n",
      "Iter:  8000 loss:  0.990885242994 val_accuracy:  0.521304260852 train_accuracy:  0.639711111111\n",
      "Iter:  8050 loss:  1.00887063272 val_accuracy:  0.533906781356 train_accuracy:  0.638022222222\n",
      "Iter:  8100 loss:  1.0455423688 val_accuracy:  0.529905981196 train_accuracy:  0.629155555556\n",
      "Iter:  8150 loss:  1.03161438764 val_accuracy:  0.531906381276 train_accuracy:  0.636844444444\n",
      "Iter:  8200 loss:  1.09580408883 val_accuracy:  0.521304260852 train_accuracy:  0.627111111111\n",
      "Iter:  8250 loss:  1.06594794628 val_accuracy:  0.520904180836 train_accuracy:  0.634422222222\n",
      "Iter:  8300 loss:  0.942352450224 val_accuracy:  0.542908581716 train_accuracy:  0.657777777778\n",
      "Iter:  8350 loss:  0.945132629955 val_accuracy:  0.55051010202 train_accuracy:  0.667066666667\n",
      "Iter:  8400 loss:  0.905210749404 val_accuracy:  0.532306461292 train_accuracy:  0.6552\n",
      "Iter:  8450 loss:  1.03926433756 val_accuracy:  0.546109221844 train_accuracy:  0.661511111111\n",
      "Iter:  8500 loss:  1.06284609481 val_accuracy:  0.520704140828 train_accuracy:  0.629377777778\n",
      "Iter:  8550 loss:  0.946930775312 val_accuracy:  0.528105621124 train_accuracy:  0.648733333333\n",
      "Iter:  8600 loss:  1.04650063945 val_accuracy:  0.524104820964 train_accuracy:  0.6278\n",
      "Iter:  8650 loss:  1.09818981955 val_accuracy:  0.516703340668 train_accuracy:  0.635266666667\n",
      "Iter:  8700 loss:  1.04811635773 val_accuracy:  0.51650330066 train_accuracy:  0.627822222222\n",
      "Iter:  8750 loss:  1.06456605122 val_accuracy:  0.533306661332 train_accuracy:  0.648577777778\n",
      "Iter:  8800 loss:  1.08163913411 val_accuracy:  0.519103820764 train_accuracy:  0.628111111111\n",
      "Iter:  8850 loss:  1.0973932174 val_accuracy:  0.538307661532 train_accuracy:  0.653333333333\n",
      "Iter:  8900 loss:  1.06361706939 val_accuracy:  0.544108821764 train_accuracy:  0.665755555556\n",
      "Iter:  8950 loss:  0.975959404463 val_accuracy:  0.504300860172 train_accuracy:  0.617911111111\n",
      "Iter:  9000 loss:  1.19505947545 val_accuracy:  0.51850370074 train_accuracy:  0.626866666667\n",
      "Iter:  9050 loss:  0.987410648968 val_accuracy:  0.530106021204 train_accuracy:  0.667555555556\n",
      "Iter:  9100 loss:  1.0249315384 val_accuracy:  0.538307661532 train_accuracy:  0.668533333333\n",
      "Iter:  9150 loss:  1.08464436918 val_accuracy:  0.508301660332 train_accuracy:  0.6196\n",
      "Iter:  9200 loss:  1.06289991355 val_accuracy:  0.499899979996 train_accuracy:  0.609133333333\n",
      "Iter:  9250 loss:  0.982827623226 val_accuracy:  0.537707541508 train_accuracy:  0.668866666667\n",
      "Iter:  9300 loss:  0.896477020819 val_accuracy:  0.542708541708 train_accuracy:  0.677488888889\n",
      "Iter:  9350 loss:  1.05731179239 val_accuracy:  0.50850170034 train_accuracy:  0.629577777778\n",
      "Iter:  9400 loss:  0.961181627099 val_accuracy:  0.544708941788 train_accuracy:  0.680377777778\n",
      "Iter:  9450 loss:  0.985153629281 val_accuracy:  0.520904180836 train_accuracy:  0.639022222222\n",
      "Iter:  9500 loss:  1.03003630632 val_accuracy:  0.523304660932 train_accuracy:  0.638222222222\n",
      "Iter:  9550 loss:  0.985713661251 val_accuracy:  0.541908381676 train_accuracy:  0.682955555556\n",
      "Iter:  9600 loss:  1.30289135657 val_accuracy:  0.509301860372 train_accuracy:  0.629088888889\n",
      "Iter:  9650 loss:  1.00814330243 val_accuracy:  0.538107621524 train_accuracy:  0.684066666667\n",
      "Iter:  9700 loss:  0.867042728159 val_accuracy:  0.52850570114 train_accuracy:  0.660933333333\n",
      "Iter:  9750 loss:  0.936494633366 val_accuracy:  0.546109221844 train_accuracy:  0.694777777778\n",
      "Iter:  9800 loss:  0.898605390323 val_accuracy:  0.547109421884 train_accuracy:  0.677\n",
      "Iter:  9850 loss:  1.05322618909 val_accuracy:  0.5475095019 train_accuracy:  0.6976\n",
      "Iter:  9900 loss:  1.04927659995 val_accuracy:  0.47849569914 train_accuracy:  0.619222222222\n",
      "Iter:  9950 loss:  0.999395595191 val_accuracy:  0.522704540908 train_accuracy:  0.669977777778\n",
      "best val accuracy:  0.55051010202 train accuacy at this point:  0.667066666667\n",
      "this is a test\n",
      "testing\n",
      "0.55051010202\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0],250,200,10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10000, alpha=0.0001, batch_size=500, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    }
   ],
   "source": [
    "y_predicted = NN.predict(X_test,'test')[1]\n",
    "save_predictions('ans1-id2303', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.45536249,  1.63894347,  3.007115  , ..., -2.5946128 ,\n",
       "        -0.10439245, -0.45016118],\n",
       "       [-0.1483906 ,  3.31136591, -1.73210326, ..., -4.38615414,\n",
       "        -2.0569141 , -1.77918121],\n",
       "       [ 1.3475619 , -0.90090121,  0.47615873, ...,  2.10323646,\n",
       "        -0.03508179,  0.80957269],\n",
       "       ..., \n",
       "       [-3.03068954, -1.94352655, -1.58373465, ..., -0.19282724,\n",
       "        -0.01620476,  3.62385487],\n",
       "       [-1.38669692,  4.47137583,  1.83075453, ...,  0.66878847,\n",
       "        -1.823124  , -1.25085093],\n",
       "       [-2.29790742,  4.55268448, -4.02390731, ..., -1.70494757,\n",
       "        -2.47889177, -0.71704684]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-id2303.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0 loss:  25.4297315977 val_accuracy:  0.133826765353 train_accuracy:  0.128133333333\n",
      "Iter:  50 loss:  25.1579672926 val_accuracy:  0.307061412282 train_accuracy:  0.299133333333\n",
      "Iter:  100 loss:  25.1592905279 val_accuracy:  0.307661532306 train_accuracy:  0.307288888889\n",
      "Iter:  150 loss:  25.1618287295 val_accuracy:  0.320264052811 train_accuracy:  0.312977777778\n",
      "Iter:  200 loss:  25.1313610233 val_accuracy:  0.340868173635 train_accuracy:  0.336488888889\n",
      "Iter:  250 loss:  25.0945239604 val_accuracy:  0.354870974195 train_accuracy:  0.351755555556\n",
      "Iter:  300 loss:  25.1083129169 val_accuracy:  0.351870374075 train_accuracy:  0.353311111111\n",
      "Iter:  350 loss:  25.0930743561 val_accuracy:  0.362672534507 train_accuracy:  0.359666666667\n",
      "Iter:  400 loss:  25.1289008552 val_accuracy:  0.367273454691 train_accuracy:  0.364066666667\n",
      "Iter:  450 loss:  25.0967990542 val_accuracy:  0.376675335067 train_accuracy:  0.374511111111\n",
      "Iter:  500 loss:  25.1778161629 val_accuracy:  0.363072614523 train_accuracy:  0.357822222222\n",
      "Iter:  550 loss:  25.1201693018 val_accuracy:  0.379075815163 train_accuracy:  0.382311111111\n",
      "Iter:  600 loss:  25.1959646647 val_accuracy:  0.405481096219 train_accuracy:  0.408977777778\n",
      "Iter:  650 loss:  25.1449887851 val_accuracy:  0.408081616323 train_accuracy:  0.410911111111\n",
      "Iter:  700 loss:  25.2380092641 val_accuracy:  0.411282256451 train_accuracy:  0.415488888889\n",
      "Iter:  750 loss:  25.350780839 val_accuracy:  0.404280856171 train_accuracy:  0.406288888889\n",
      "Iter:  800 loss:  25.1577388475 val_accuracy:  0.417283456691 train_accuracy:  0.417822222222\n",
      "Iter:  850 loss:  25.3321769877 val_accuracy:  0.402280456091 train_accuracy:  0.409422222222\n",
      "Iter:  900 loss:  25.3113243583 val_accuracy:  0.407681536307 train_accuracy:  0.412466666667\n",
      "Iter:  950 loss:  25.3949647168 val_accuracy:  0.422484496899 train_accuracy:  0.4292\n",
      "Iter:  1000 loss:  25.2823178684 val_accuracy:  0.424084816963 train_accuracy:  0.430288888889\n",
      "Iter:  1050 loss:  25.398317163 val_accuracy:  0.422284456891 train_accuracy:  0.4354\n",
      "Iter:  1100 loss:  25.3420419064 val_accuracy:  0.423084616923 train_accuracy:  0.424911111111\n",
      "Iter:  1150 loss:  25.4496841063 val_accuracy:  0.426685337067 train_accuracy:  0.434533333333\n",
      "Iter:  1200 loss:  25.4738696924 val_accuracy:  0.455891178236 train_accuracy:  0.453511111111\n",
      "Iter:  1250 loss:  25.5081504011 val_accuracy:  0.432286457291 train_accuracy:  0.434666666667\n",
      "Iter:  1300 loss:  25.424930774 val_accuracy:  0.441088217644 train_accuracy:  0.4496\n",
      "Iter:  1350 loss:  25.5800945148 val_accuracy:  0.420484096819 train_accuracy:  0.430533333333\n",
      "Iter:  1400 loss:  25.4270554231 val_accuracy:  0.450090018004 train_accuracy:  0.454\n",
      "Iter:  1450 loss:  25.5213174212 val_accuracy:  0.453690738148 train_accuracy:  0.462955555556\n",
      "Iter:  1500 loss:  25.6060579572 val_accuracy:  0.450290058012 train_accuracy:  0.454822222222\n",
      "Iter:  1550 loss:  25.4853160989 val_accuracy:  0.462692538508 train_accuracy:  0.467555555556\n",
      "Iter:  1600 loss:  25.5893492615 val_accuracy:  0.425085017003 train_accuracy:  0.429088888889\n",
      "Iter:  1650 loss:  25.5274721389 val_accuracy:  0.467693538708 train_accuracy:  0.478288888889\n",
      "Iter:  1700 loss:  25.6290375705 val_accuracy:  0.467893578716 train_accuracy:  0.471622222222\n",
      "Iter:  1750 loss:  25.6986341411 val_accuracy:  0.442688537708 train_accuracy:  0.451688888889\n",
      "Iter:  1800 loss:  25.6534355947 val_accuracy:  0.468693738748 train_accuracy:  0.4804\n",
      "Iter:  1850 loss:  25.7906640167 val_accuracy:  0.461692338468 train_accuracy:  0.469177777778\n",
      "Iter:  1900 loss:  25.6164132577 val_accuracy:  0.470294058812 train_accuracy:  0.482577777778\n",
      "Iter:  1950 loss:  25.803258748 val_accuracy:  0.471894378876 train_accuracy:  0.477644444444\n",
      "Iter:  2000 loss:  25.8423692797 val_accuracy:  0.478895779156 train_accuracy:  0.479777777778\n",
      "Iter:  2050 loss:  25.7820375864 val_accuracy:  0.480096019204 train_accuracy:  0.480422222222\n",
      "Iter:  2100 loss:  25.8677227042 val_accuracy:  0.449289857972 train_accuracy:  0.471422222222\n",
      "Iter:  2150 loss:  25.8439886662 val_accuracy:  0.45849169834 train_accuracy:  0.4664\n",
      "Iter:  2200 loss:  25.9948738974 val_accuracy:  0.468693738748 train_accuracy:  0.48\n",
      "Iter:  2250 loss:  25.8474456528 val_accuracy:  0.472694538908 train_accuracy:  0.483133333333\n",
      "Iter:  2300 loss:  25.8326228109 val_accuracy:  0.485297059412 train_accuracy:  0.500066666667\n",
      "Iter:  2350 loss:  25.9107712225 val_accuracy:  0.480896179236 train_accuracy:  0.488955555556\n",
      "Iter:  2400 loss:  26.018107764 val_accuracy:  0.487097419484 train_accuracy:  0.501155555556\n",
      "Iter:  2450 loss:  26.0106923095 val_accuracy:  0.476095219044 train_accuracy:  0.492866666667\n",
      "Iter:  2500 loss:  26.0018082469 val_accuracy:  0.471894378876 train_accuracy:  0.490533333333\n",
      "Iter:  2550 loss:  26.0296095412 val_accuracy:  0.49049809962 train_accuracy:  0.5058\n",
      "Iter:  2600 loss:  26.0482419598 val_accuracy:  0.484696939388 train_accuracy:  0.495488888889\n",
      "Iter:  2650 loss:  26.1993440627 val_accuracy:  0.494698939788 train_accuracy:  0.508977777778\n",
      "Iter:  2700 loss:  26.0907930356 val_accuracy:  0.489897979596 train_accuracy:  0.508577777778\n",
      "Iter:  2750 loss:  26.1220392888 val_accuracy:  0.489897979596 train_accuracy:  0.502444444444\n",
      "Iter:  2800 loss:  26.1631016297 val_accuracy:  0.492898579716 train_accuracy:  0.513266666667\n",
      "Iter:  2850 loss:  26.1052177876 val_accuracy:  0.488097619524 train_accuracy:  0.508666666667\n",
      "Iter:  2900 loss:  26.1698420224 val_accuracy:  0.49649929986 train_accuracy:  0.513933333333\n",
      "Iter:  2950 loss:  26.2482486578 val_accuracy:  0.483896779356 train_accuracy:  0.505511111111\n",
      "Iter:  3000 loss:  26.1916224145 val_accuracy:  0.476895379076 train_accuracy:  0.502244444444\n",
      "Iter:  3050 loss:  26.343710554 val_accuracy:  0.489697939588 train_accuracy:  0.509866666667\n",
      "Iter:  3100 loss:  26.2825705323 val_accuracy:  0.498899779956 train_accuracy:  0.519377777778\n",
      "Iter:  3150 loss:  26.3075249253 val_accuracy:  0.501700340068 train_accuracy:  0.521666666667\n",
      "Iter:  3200 loss:  26.3410302929 val_accuracy:  0.48649729946 train_accuracy:  0.511866666667\n",
      "Iter:  3250 loss:  26.4118355923 val_accuracy:  0.487097419484 train_accuracy:  0.510733333333\n",
      "Iter:  3300 loss:  26.4440609487 val_accuracy:  0.47849569914 train_accuracy:  0.498133333333\n",
      "Iter:  3350 loss:  26.4928727772 val_accuracy:  0.507701540308 train_accuracy:  0.529\n",
      "Iter:  3400 loss:  26.5564418858 val_accuracy:  0.49649929986 train_accuracy:  0.517066666667\n",
      "Iter:  3450 loss:  26.5158408206 val_accuracy:  0.496699339868 train_accuracy:  0.516577777778\n",
      "Iter:  3500 loss:  26.5970089964 val_accuracy:  0.49549909982 train_accuracy:  0.518244444444\n",
      "Iter:  3550 loss:  26.6167603202 val_accuracy:  0.485297059412 train_accuracy:  0.512177777778\n",
      "Iter:  3600 loss:  26.5421824759 val_accuracy:  0.507701540308 train_accuracy:  0.533955555556\n",
      "Iter:  3650 loss:  26.6281690886 val_accuracy:  0.498699739948 train_accuracy:  0.524533333333\n",
      "Iter:  3700 loss:  26.6503097053 val_accuracy:  0.486097219444 train_accuracy:  0.505444444444\n",
      "Iter:  3750 loss:  26.5977000728 val_accuracy:  0.50150030006 train_accuracy:  0.524933333333\n",
      "Iter:  3800 loss:  26.7176141512 val_accuracy:  0.488897779556 train_accuracy:  0.518066666667\n",
      "Iter:  3850 loss:  26.761321896 val_accuracy:  0.491698339668 train_accuracy:  0.517977777778\n",
      "Iter:  3900 loss:  26.7003754575 val_accuracy:  0.51550310062 train_accuracy:  0.541333333333\n",
      "Iter:  3950 loss:  26.7680411636 val_accuracy:  0.511902380476 train_accuracy:  0.5358\n",
      "Iter:  4000 loss:  26.7333322524 val_accuracy:  0.511702340468 train_accuracy:  0.543333333333\n",
      "Iter:  4050 loss:  26.8036998948 val_accuracy:  0.518903780756 train_accuracy:  0.541222222222\n",
      "Iter:  4100 loss:  26.8511809577 val_accuracy:  0.502300460092 train_accuracy:  0.535111111111\n",
      "Iter:  4150 loss:  26.8680329182 val_accuracy:  0.503700740148 train_accuracy:  0.541688888889\n",
      "Iter:  4200 loss:  26.9381266832 val_accuracy:  0.497299459892 train_accuracy:  0.526266666667\n",
      "Iter:  4250 loss:  26.8711230694 val_accuracy:  0.517903580716 train_accuracy:  0.550088888889\n",
      "Iter:  4300 loss:  26.964179219 val_accuracy:  0.492298459692 train_accuracy:  0.525\n",
      "Iter:  4350 loss:  26.9720149409 val_accuracy:  0.50350070014 train_accuracy:  0.537222222222\n",
      "Iter:  4400 loss:  27.0371355578 val_accuracy:  0.519903980796 train_accuracy:  0.550022222222\n",
      "Iter:  4450 loss:  27.0514767364 val_accuracy:  0.518903780756 train_accuracy:  0.553622222222\n",
      "Iter:  4500 loss:  27.1351247259 val_accuracy:  0.50950190038 train_accuracy:  0.537111111111\n",
      "Iter:  4550 loss:  27.1884429145 val_accuracy:  0.493298659732 train_accuracy:  0.530444444444\n",
      "Iter:  4600 loss:  27.052486316 val_accuracy:  0.509901980396 train_accuracy:  0.544933333333\n",
      "Iter:  4650 loss:  27.1522103796 val_accuracy:  0.504300860172 train_accuracy:  0.542666666667\n",
      "Iter:  4700 loss:  27.2433586822 val_accuracy:  0.508901780356 train_accuracy:  0.5378\n",
      "Iter:  4750 loss:  27.3045533146 val_accuracy:  0.507701540308 train_accuracy:  0.541\n",
      "Iter:  4800 loss:  27.3166122836 val_accuracy:  0.494898979796 train_accuracy:  0.527622222222\n",
      "Iter:  4850 loss:  27.3418139224 val_accuracy:  0.512302460492 train_accuracy:  0.535066666667\n",
      "Iter:  4900 loss:  27.3571667332 val_accuracy:  0.499699939988 train_accuracy:  0.534022222222\n",
      "Iter:  4950 loss:  27.2821831234 val_accuracy:  0.519703940788 train_accuracy:  0.554066666667\n",
      "Iter:  5000 loss:  27.5120353972 val_accuracy:  0.515903180636 train_accuracy:  0.548533333333\n",
      "Iter:  5050 loss:  27.3758282769 val_accuracy:  0.523704740948 train_accuracy:  0.559888888889\n",
      "Iter:  5100 loss:  27.4981773131 val_accuracy:  0.500100020004 train_accuracy:  0.534733333333\n",
      "Iter:  5150 loss:  27.419781846 val_accuracy:  0.521704340868 train_accuracy:  0.556111111111\n",
      "Iter:  5200 loss:  27.5952084945 val_accuracy:  0.52050410082 train_accuracy:  0.5626\n",
      "Iter:  5250 loss:  27.5657053884 val_accuracy:  0.510102020404 train_accuracy:  0.554911111111\n",
      "Iter:  5300 loss:  27.4840385253 val_accuracy:  0.529905981196 train_accuracy:  0.573355555556\n",
      "Iter:  5350 loss:  27.6297665676 val_accuracy:  0.521904380876 train_accuracy:  0.5562\n",
      "Iter:  5400 loss:  27.5955804762 val_accuracy:  0.52150430086 train_accuracy:  0.557377777778\n",
      "Iter:  5450 loss:  27.7117885152 val_accuracy:  0.508701740348 train_accuracy:  0.549577777778\n",
      "Iter:  5500 loss:  27.7075439628 val_accuracy:  0.5125025005 train_accuracy:  0.551977777778\n",
      "Iter:  5550 loss:  27.6692367997 val_accuracy:  0.518303660732 train_accuracy:  0.563088888889\n",
      "Iter:  5600 loss:  27.7285986312 val_accuracy:  0.52450490098 train_accuracy:  0.565711111111\n",
      "Iter:  5650 loss:  27.8230060064 val_accuracy:  0.523104620924 train_accuracy:  0.565844444444\n",
      "Iter:  5700 loss:  27.8028362093 val_accuracy:  0.526705341068 train_accuracy:  0.562711111111\n",
      "Iter:  5750 loss:  27.9113983532 val_accuracy:  0.521904380876 train_accuracy:  0.562577777778\n",
      "Iter:  5800 loss:  27.9164465931 val_accuracy:  0.52350470094 train_accuracy:  0.573377777778\n",
      "Iter:  5850 loss:  27.8294741111 val_accuracy:  0.527105421084 train_accuracy:  0.5674\n",
      "Iter:  5900 loss:  27.9170437015 val_accuracy:  0.518703740748 train_accuracy:  0.569111111111\n",
      "Iter:  5950 loss:  27.9135269005 val_accuracy:  0.516103220644 train_accuracy:  0.563222222222\n",
      "Iter:  6000 loss:  28.1789762492 val_accuracy:  0.50150030006 train_accuracy:  0.555511111111\n",
      "Iter:  6050 loss:  28.1369546022 val_accuracy:  0.51550310062 train_accuracy:  0.5572\n",
      "Iter:  6100 loss:  28.088730189 val_accuracy:  0.515103020604 train_accuracy:  0.562266666667\n",
      "Iter:  6150 loss:  28.0679682412 val_accuracy:  0.517703540708 train_accuracy:  0.5624\n",
      "Iter:  6200 loss:  28.1774524101 val_accuracy:  0.529705941188 train_accuracy:  0.575155555556\n",
      "Iter:  6250 loss:  28.2041204278 val_accuracy:  0.528105621124 train_accuracy:  0.573488888889\n",
      "Iter:  6300 loss:  28.2166878548 val_accuracy:  0.533306661332 train_accuracy:  0.586555555556\n",
      "Iter:  6350 loss:  28.3395593639 val_accuracy:  0.533106621324 train_accuracy:  0.582111111111\n",
      "Iter:  6400 loss:  28.3879325341 val_accuracy:  0.519303860772 train_accuracy:  0.566888888889\n",
      "Iter:  6450 loss:  28.3226539521 val_accuracy:  0.530706141228 train_accuracy:  0.571933333333\n",
      "Iter:  6500 loss:  28.3764556999 val_accuracy:  0.5125025005 train_accuracy:  0.563844444444\n",
      "Iter:  6550 loss:  28.4290029763 val_accuracy:  0.525905181036 train_accuracy:  0.574355555556\n",
      "Iter:  6600 loss:  28.4268100123 val_accuracy:  0.519903980796 train_accuracy:  0.569888888889\n",
      "Iter:  6650 loss:  28.4002115016 val_accuracy:  0.51150230046 train_accuracy:  0.563355555556\n",
      "Iter:  6700 loss:  28.5566538344 val_accuracy:  0.535107021404 train_accuracy:  0.586422222222\n",
      "Iter:  6750 loss:  28.4978746593 val_accuracy:  0.52650530106 train_accuracy:  0.5844\n",
      "Iter:  6800 loss:  28.6440376291 val_accuracy:  0.525105021004 train_accuracy:  0.580777777778\n",
      "Iter:  6850 loss:  28.612675036 val_accuracy:  0.524704940988 train_accuracy:  0.570866666667\n",
      "Iter:  6900 loss:  28.5966858771 val_accuracy:  0.520304060812 train_accuracy:  0.5652\n",
      "Iter:  6950 loss:  28.6899362811 val_accuracy:  0.530706141228 train_accuracy:  0.579133333333\n",
      "Iter:  7000 loss:  28.6537365603 val_accuracy:  0.517903580716 train_accuracy:  0.566511111111\n",
      "Iter:  7050 loss:  28.7850428246 val_accuracy:  0.53550710142 train_accuracy:  0.592977777778\n",
      "Iter:  7100 loss:  28.779894876 val_accuracy:  0.53850770154 train_accuracy:  0.591688888889\n",
      "Iter:  7150 loss:  28.8169407197 val_accuracy:  0.523304660932 train_accuracy:  0.5786\n",
      "Iter:  7200 loss:  28.7629992943 val_accuracy:  0.542308461692 train_accuracy:  0.599066666667\n",
      "Iter:  7250 loss:  28.9459607338 val_accuracy:  0.53050610122 train_accuracy:  0.586066666667\n",
      "Iter:  7300 loss:  28.9611930549 val_accuracy:  0.516903380676 train_accuracy:  0.5698\n",
      "Iter:  7350 loss:  29.0233014235 val_accuracy:  0.524104820964 train_accuracy:  0.582755555556\n",
      "Iter:  7400 loss:  29.025185082 val_accuracy:  0.529305861172 train_accuracy:  0.590644444444\n",
      "Iter:  7450 loss:  29.1300518373 val_accuracy:  0.53550710142 train_accuracy:  0.590333333333\n",
      "Iter:  7500 loss:  29.0247005291 val_accuracy:  0.538707741548 train_accuracy:  0.598022222222\n",
      "Iter:  7550 loss:  29.1488680205 val_accuracy:  0.496899379876 train_accuracy:  0.549511111111\n",
      "Iter:  7600 loss:  29.0626852218 val_accuracy:  0.543908781756 train_accuracy:  0.596844444444\n",
      "Iter:  7650 loss:  29.136152944 val_accuracy:  0.544908981796 train_accuracy:  0.607911111111\n",
      "Iter:  7700 loss:  29.1975727778 val_accuracy:  0.547309461892 train_accuracy:  0.608666666667\n",
      "Iter:  7750 loss:  29.2188871819 val_accuracy:  0.530706141228 train_accuracy:  0.583066666667\n",
      "Iter:  7800 loss:  29.2697916013 val_accuracy:  0.543308661732 train_accuracy:  0.613\n",
      "Iter:  7850 loss:  29.3658383992 val_accuracy:  0.522104420884 train_accuracy:  0.578755555556\n",
      "Iter:  7900 loss:  29.3345299228 val_accuracy:  0.524704940988 train_accuracy:  0.587866666667\n",
      "Iter:  7950 loss:  29.4198975854 val_accuracy:  0.506701340268 train_accuracy:  0.565222222222\n",
      "Iter:  8000 loss:  29.3522334386 val_accuracy:  0.550710142028 train_accuracy:  0.616044444444\n",
      "Iter:  8050 loss:  29.409776123 val_accuracy:  0.532106421284 train_accuracy:  0.592266666667\n",
      "Iter:  8100 loss:  29.521945978 val_accuracy:  0.539307861572 train_accuracy:  0.602577777778\n",
      "Iter:  8150 loss:  29.5630260775 val_accuracy:  0.514302860572 train_accuracy:  0.571288888889\n",
      "Iter:  8200 loss:  29.6470792467 val_accuracy:  0.542108421684 train_accuracy:  0.606777777778\n",
      "Iter:  8250 loss:  29.6137725622 val_accuracy:  0.522904580916 train_accuracy:  0.589733333333\n",
      "Iter:  8300 loss:  29.6237969074 val_accuracy:  0.522704540908 train_accuracy:  0.582422222222\n",
      "Iter:  8350 loss:  29.6176661393 val_accuracy:  0.536307261452 train_accuracy:  0.603755555556\n",
      "Iter:  8400 loss:  29.6393949178 val_accuracy:  0.542308461692 train_accuracy:  0.608711111111\n",
      "Iter:  8450 loss:  29.7887988922 val_accuracy:  0.534106821364 train_accuracy:  0.604088888889\n",
      "Iter:  8500 loss:  29.7893672459 val_accuracy:  0.54550910182 train_accuracy:  0.6136\n",
      "Iter:  8550 loss:  29.8076572412 val_accuracy:  0.533306661332 train_accuracy:  0.599644444444\n",
      "Iter:  8600 loss:  29.9477039221 val_accuracy:  0.510702140428 train_accuracy:  0.570955555556\n",
      "Iter:  8650 loss:  29.9413108064 val_accuracy:  0.55151030206 train_accuracy:  0.623377777778\n",
      "Iter:  8700 loss:  29.9150164234 val_accuracy:  0.551310262052 train_accuracy:  0.620555555556\n",
      "Iter:  8750 loss:  30.0968941429 val_accuracy:  0.529105821164 train_accuracy:  0.598133333333\n",
      "Iter:  8800 loss:  30.1099458058 val_accuracy:  0.529105821164 train_accuracy:  0.595155555556\n",
      "Iter:  8850 loss:  30.1402205699 val_accuracy:  0.532306461292 train_accuracy:  0.597755555556\n",
      "Iter:  8900 loss:  30.1795140113 val_accuracy:  0.542308461692 train_accuracy:  0.612422222222\n",
      "Iter:  8950 loss:  30.1243833086 val_accuracy:  0.538907781556 train_accuracy:  0.603866666667\n",
      "Iter:  9000 loss:  30.3333724507 val_accuracy:  0.52850570114 train_accuracy:  0.601866666667\n",
      "Iter:  9050 loss:  30.1815539125 val_accuracy:  0.55351070214 train_accuracy:  0.632133333333\n",
      "Iter:  9100 loss:  30.2229745803 val_accuracy:  0.538307661532 train_accuracy:  0.607422222222\n",
      "Iter:  9150 loss:  30.3715731371 val_accuracy:  0.51450290058 train_accuracy:  0.570444444444\n",
      "Iter:  9200 loss:  30.4141025674 val_accuracy:  0.538107621524 train_accuracy:  0.609177777778\n",
      "Iter:  9250 loss:  30.3482455816 val_accuracy:  0.535307061412 train_accuracy:  0.603666666667\n",
      "Iter:  9300 loss:  30.4143955236 val_accuracy:  0.54550910182 train_accuracy:  0.6148\n",
      "Iter:  9350 loss:  30.4850549885 val_accuracy:  0.528905781156 train_accuracy:  0.604844444444\n",
      "Iter:  9400 loss:  30.5422930712 val_accuracy:  0.542908581716 train_accuracy:  0.617933333333\n",
      "Iter:  9450 loss:  30.5918868151 val_accuracy:  0.533906781356 train_accuracy:  0.603666666667\n",
      "Iter:  9500 loss:  30.6494671843 val_accuracy:  0.534906981396 train_accuracy:  0.608111111111\n",
      "Iter:  9550 loss:  30.6681029393 val_accuracy:  0.546309261852 train_accuracy:  0.621066666667\n",
      "Iter:  9600 loss:  30.8720406719 val_accuracy:  0.53850770154 train_accuracy:  0.614333333333\n",
      "Iter:  9650 loss:  30.7759194698 val_accuracy:  0.54950990198 train_accuracy:  0.630911111111\n",
      "Iter:  9700 loss:  30.7061281795 val_accuracy:  0.534106821364 train_accuracy:  0.613822222222\n",
      "Iter:  9750 loss:  30.7609779459 val_accuracy:  0.548709741948 train_accuracy:  0.628777777778\n",
      "Iter:  9800 loss:  30.763395438 val_accuracy:  0.540708141628 train_accuracy:  0.6206\n",
      "Iter:  9850 loss:  30.8970252322 val_accuracy:  0.550910182036 train_accuracy:  0.636911111111\n",
      "Iter:  9900 loss:  30.8595573183 val_accuracy:  0.528905781156 train_accuracy:  0.608266666667\n",
      "Iter:  9950 loss:  30.9928257178 val_accuracy:  0.524704940988 train_accuracy:  0.596088888889\n",
      "Iter:  10000 loss:  31.060140702 val_accuracy:  0.525905181036 train_accuracy:  0.596088888889\n",
      "Iter:  10050 loss:  31.0493521963 val_accuracy:  0.55051010202 train_accuracy:  0.634977777778\n",
      "Iter:  10100 loss:  31.1806239959 val_accuracy:  0.54850970194 train_accuracy:  0.639377777778\n",
      "Iter:  10150 loss:  31.1208198294 val_accuracy:  0.53850770154 train_accuracy:  0.619311111111\n",
      "Iter:  10200 loss:  31.1445685632 val_accuracy:  0.548309661932 train_accuracy:  0.644466666667\n",
      "Iter:  10250 loss:  31.2737878409 val_accuracy:  0.538907781556 train_accuracy:  0.619511111111\n",
      "Iter:  10300 loss:  31.2112343992 val_accuracy:  0.544708941788 train_accuracy:  0.6352\n",
      "Iter:  10350 loss:  31.2613407354 val_accuracy:  0.529905981196 train_accuracy:  0.620911111111\n",
      "Iter:  10400 loss:  31.3633534849 val_accuracy:  0.526905381076 train_accuracy:  0.608111111111\n",
      "Iter:  10450 loss:  31.344543728 val_accuracy:  0.54550910182 train_accuracy:  0.627311111111\n",
      "Iter:  10500 loss:  31.4200484703 val_accuracy:  0.549709941988 train_accuracy:  0.641577777778\n",
      "Iter:  10550 loss:  31.4296888092 val_accuracy:  0.535307061412 train_accuracy:  0.618888888889\n",
      "Iter:  10600 loss:  31.5158209312 val_accuracy:  0.546109221844 train_accuracy:  0.633688888889\n",
      "Iter:  10650 loss:  31.5646724746 val_accuracy:  0.544708941788 train_accuracy:  0.622044444444\n",
      "Iter:  10700 loss:  31.5512012884 val_accuracy:  0.536707341468 train_accuracy:  0.619511111111\n",
      "Iter:  10750 loss:  31.5480547772 val_accuracy:  0.549309861972 train_accuracy:  0.636666666667\n",
      "Iter:  10800 loss:  31.6225602335 val_accuracy:  0.555911182236 train_accuracy:  0.644955555556\n",
      "Iter:  10850 loss:  31.7210428182 val_accuracy:  0.515703140628 train_accuracy:  0.600155555556\n",
      "Iter:  10900 loss:  31.6858859657 val_accuracy:  0.555711142228 train_accuracy:  0.648577777778\n",
      "Iter:  10950 loss:  31.8722126619 val_accuracy:  0.544908981796 train_accuracy:  0.6384\n",
      "Iter:  11000 loss:  31.8621503745 val_accuracy:  0.552110422084 train_accuracy:  0.645644444444\n",
      "Iter:  11050 loss:  31.8839254776 val_accuracy:  0.55351070214 train_accuracy:  0.650866666667\n",
      "Iter:  11100 loss:  31.8288043566 val_accuracy:  0.553910782156 train_accuracy:  0.647755555556\n",
      "Iter:  11150 loss:  31.9921590952 val_accuracy:  0.54050810162 train_accuracy:  0.630911111111\n",
      "Iter:  11200 loss:  32.0780554889 val_accuracy:  0.556911382276 train_accuracy:  0.639511111111\n",
      "Iter:  11250 loss:  32.0210901492 val_accuracy:  0.538307661532 train_accuracy:  0.632777777778\n",
      "Iter:  11300 loss:  32.1277421128 val_accuracy:  0.547909581916 train_accuracy:  0.646244444444\n",
      "Iter:  11350 loss:  32.1255600731 val_accuracy:  0.53650730146 train_accuracy:  0.623955555556\n",
      "Iter:  11400 loss:  32.1579579346 val_accuracy:  0.527905581116 train_accuracy:  0.616222222222\n",
      "Iter:  11450 loss:  32.294477215 val_accuracy:  0.549309861972 train_accuracy:  0.648333333333\n",
      "Iter:  11500 loss:  32.1598089287 val_accuracy:  0.545309061812 train_accuracy:  0.638177777778\n",
      "Iter:  11550 loss:  32.3738967669 val_accuracy:  0.5375075015 train_accuracy:  0.6322\n",
      "Iter:  11600 loss:  32.2665927324 val_accuracy:  0.553110622124 train_accuracy:  0.651044444444\n",
      "Iter:  11650 loss:  32.4514759707 val_accuracy:  0.531306261252 train_accuracy:  0.616244444444\n",
      "Iter:  11700 loss:  32.468284291 val_accuracy:  0.558111622324 train_accuracy:  0.660266666667\n",
      "Iter:  11750 loss:  32.5080410862 val_accuracy:  0.548709741948 train_accuracy:  0.646333333333\n",
      "Iter:  11800 loss:  32.4826071855 val_accuracy:  0.554310862172 train_accuracy:  0.645088888889\n",
      "Iter:  11850 loss:  32.6626188886 val_accuracy:  0.5275055011 train_accuracy:  0.616044444444\n",
      "Iter:  11900 loss:  32.6601735976 val_accuracy:  0.547709541908 train_accuracy:  0.644711111111\n",
      "Iter:  11950 loss:  32.7502365082 val_accuracy:  0.540308061612 train_accuracy:  0.636266666667\n",
      "Iter:  12000 loss:  32.6720794278 val_accuracy:  0.553910782156 train_accuracy:  0.653533333333\n",
      "Iter:  12050 loss:  32.7547393415 val_accuracy:  0.504100820164 train_accuracy:  0.590088888889\n",
      "Iter:  12100 loss:  32.8338657779 val_accuracy:  0.527105421084 train_accuracy:  0.625177777778\n",
      "Iter:  12150 loss:  32.867082212 val_accuracy:  0.548109621924 train_accuracy:  0.645422222222\n",
      "Iter:  12200 loss:  32.7935999814 val_accuracy:  0.559111822364 train_accuracy:  0.659911111111\n",
      "Iter:  12250 loss:  32.9469685779 val_accuracy:  0.543108621724 train_accuracy:  0.646666666667\n",
      "Iter:  12300 loss:  32.9473118848 val_accuracy:  0.561912382476 train_accuracy:  0.667644444444\n",
      "Iter:  12350 loss:  33.0143658654 val_accuracy:  0.552910582116 train_accuracy:  0.662155555556\n",
      "Iter:  12400 loss:  32.9839428172 val_accuracy:  0.550710142028 train_accuracy:  0.647955555556\n",
      "Iter:  12450 loss:  33.0722315327 val_accuracy:  0.557111422284 train_accuracy:  0.663822222222\n",
      "Iter:  12500 loss:  33.1823353592 val_accuracy:  0.543708741748 train_accuracy:  0.638733333333\n",
      "Iter:  12550 loss:  33.2454870582 val_accuracy:  0.557111422284 train_accuracy:  0.666844444444\n",
      "Iter:  12600 loss:  33.2365056259 val_accuracy:  0.545709141828 train_accuracy:  0.653444444444\n",
      "Iter:  12650 loss:  33.2485237407 val_accuracy:  0.546909381876 train_accuracy:  0.647044444444\n",
      "Iter:  12700 loss:  33.3395030266 val_accuracy:  0.539907981596 train_accuracy:  0.633555555556\n",
      "Iter:  12750 loss:  33.434641383 val_accuracy:  0.528305661132 train_accuracy:  0.629022222222\n",
      "Iter:  12800 loss:  33.5090314726 val_accuracy:  0.499899979996 train_accuracy:  0.584511111111\n",
      "Iter:  12850 loss:  33.496988017 val_accuracy:  0.556311262252 train_accuracy:  0.670511111111\n",
      "Iter:  12900 loss:  33.4755716834 val_accuracy:  0.53850770154 train_accuracy:  0.646133333333\n",
      "Iter:  12950 loss:  33.5874625303 val_accuracy:  0.550110022004 train_accuracy:  0.657888888889\n",
      "Iter:  13000 loss:  33.5973123499 val_accuracy:  0.551710342068 train_accuracy:  0.6678\n",
      "Iter:  13050 loss:  33.6191967921 val_accuracy:  0.554710942188 train_accuracy:  0.674688888889\n",
      "Iter:  13100 loss:  33.7588751073 val_accuracy:  0.53850770154 train_accuracy:  0.639511111111\n",
      "Iter:  13150 loss:  33.7844816928 val_accuracy:  0.55151030206 train_accuracy:  0.663844444444\n",
      "Iter:  13200 loss:  33.7918176398 val_accuracy:  0.540908181636 train_accuracy:  0.644444444444\n",
      "Iter:  13250 loss:  33.840598473 val_accuracy:  0.539907981596 train_accuracy:  0.653933333333\n",
      "Iter:  13300 loss:  33.8688076855 val_accuracy:  0.555911182236 train_accuracy:  0.668355555556\n",
      "Iter:  13350 loss:  33.9016243818 val_accuracy:  0.549309861972 train_accuracy:  0.6626\n",
      "Iter:  13400 loss:  34.0476214925 val_accuracy:  0.533306661332 train_accuracy:  0.637577777778\n",
      "Iter:  13450 loss:  33.9585342606 val_accuracy:  0.547109421884 train_accuracy:  0.658577777778\n",
      "Iter:  13500 loss:  34.0716978742 val_accuracy:  0.54850970194 train_accuracy:  0.652333333333\n",
      "Iter:  13550 loss:  34.1925530932 val_accuracy:  0.556711342268 train_accuracy:  0.6748\n",
      "Iter:  13600 loss:  34.257427953 val_accuracy:  0.5175035007 train_accuracy:  0.631866666667\n",
      "Iter:  13650 loss:  34.21970671 val_accuracy:  0.55951190238 train_accuracy:  0.682044444444\n",
      "Iter:  13700 loss:  34.1244912631 val_accuracy:  0.554710942188 train_accuracy:  0.672533333333\n",
      "Iter:  13750 loss:  34.2800331715 val_accuracy:  0.547909581916 train_accuracy:  0.6602\n",
      "Iter:  13800 loss:  34.3199755601 val_accuracy:  0.568913782757 train_accuracy:  0.684955555556\n",
      "Iter:  13850 loss:  34.3612247984 val_accuracy:  0.540908181636 train_accuracy:  0.652511111111\n",
      "Iter:  13900 loss:  34.3867260919 val_accuracy:  0.565713142629 train_accuracy:  0.683933333333\n",
      "Iter:  13950 loss:  34.4478336714 val_accuracy:  0.546109221844 train_accuracy:  0.655733333333\n",
      "Iter:  14000 loss:  34.5692608362 val_accuracy:  0.563512702541 train_accuracy:  0.676822222222\n",
      "Iter:  14050 loss:  34.6051619256 val_accuracy:  0.539107821564 train_accuracy:  0.6498\n",
      "Iter:  14100 loss:  34.6161667775 val_accuracy:  0.562312462492 train_accuracy:  0.6854\n",
      "Iter:  14150 loss:  34.6101132115 val_accuracy:  0.557311462292 train_accuracy:  0.6788\n",
      "Iter:  14200 loss:  34.6202568788 val_accuracy:  0.554310862172 train_accuracy:  0.675422222222\n",
      "Iter:  14250 loss:  34.8170331913 val_accuracy:  0.547109421884 train_accuracy:  0.660333333333\n",
      "Iter:  14300 loss:  34.7713988799 val_accuracy:  0.560912182436 train_accuracy:  0.6844\n",
      "Iter:  14350 loss:  34.8898704938 val_accuracy:  0.552110422084 train_accuracy:  0.671511111111\n",
      "Iter:  14400 loss:  34.8492544395 val_accuracy:  0.55951190238 train_accuracy:  0.686911111111\n",
      "Iter:  14450 loss:  34.8437024613 val_accuracy:  0.556911382276 train_accuracy:  0.6876\n",
      "Iter:  14500 loss:  35.0310299617 val_accuracy:  0.551110222044 train_accuracy:  0.666755555556\n",
      "Iter:  14550 loss:  34.9884212024 val_accuracy:  0.552710542108 train_accuracy:  0.680355555556\n",
      "Iter:  14600 loss:  35.0791716151 val_accuracy:  0.550710142028 train_accuracy:  0.674288888889\n",
      "Iter:  14650 loss:  35.272225358 val_accuracy:  0.534706941388 train_accuracy:  0.654133333333\n",
      "Iter:  14700 loss:  35.1839928373 val_accuracy:  0.556911382276 train_accuracy:  0.684288888889\n",
      "Iter:  14750 loss:  35.2069129952 val_accuracy:  0.555711142228 train_accuracy:  0.680466666667\n",
      "Iter:  14800 loss:  35.4166389692 val_accuracy:  0.545909181836 train_accuracy:  0.6782\n",
      "Iter:  14850 loss:  35.2414179281 val_accuracy:  0.563912782557 train_accuracy:  0.690266666667\n",
      "Iter:  14900 loss:  35.3133446674 val_accuracy:  0.55351070214 train_accuracy:  0.683111111111\n",
      "Iter:  14950 loss:  35.4181816576 val_accuracy:  0.541908381676 train_accuracy:  0.656066666667\n",
      "Iter:  15000 loss:  35.4185541283 val_accuracy:  0.564712942589 train_accuracy:  0.7026\n",
      "Iter:  15050 loss:  35.3718415161 val_accuracy:  0.574314862973 train_accuracy:  0.714311111111\n",
      "Iter:  15100 loss:  35.3565994284 val_accuracy:  0.577715543109 train_accuracy:  0.714466666667\n",
      "Iter:  15150 loss:  35.4007203678 val_accuracy:  0.573514702941 train_accuracy:  0.715777777778\n",
      "Iter:  15200 loss:  35.3006744252 val_accuracy:  0.572114422885 train_accuracy:  0.714311111111\n",
      "Iter:  15250 loss:  35.3719709069 val_accuracy:  0.574914982997 train_accuracy:  0.717022222222\n",
      "Iter:  15300 loss:  35.3777901683 val_accuracy:  0.575515103021 train_accuracy:  0.716088888889\n",
      "Iter:  15350 loss:  35.4190824812 val_accuracy:  0.573314662933 train_accuracy:  0.715266666667\n",
      "Iter:  15400 loss:  35.447406154 val_accuracy:  0.575115023005 train_accuracy:  0.715977777778\n",
      "Iter:  15450 loss:  35.3885432474 val_accuracy:  0.575515103021 train_accuracy:  0.717844444444\n",
      "Iter:  15500 loss:  35.3601691468 val_accuracy:  0.574314862973 train_accuracy:  0.716577777778\n",
      "Iter:  15550 loss:  35.4810298978 val_accuracy:  0.574714942989 train_accuracy:  0.715111111111\n",
      "Iter:  15600 loss:  35.3702689511 val_accuracy:  0.575915183037 train_accuracy:  0.717888888889\n",
      "Iter:  15650 loss:  35.3826013371 val_accuracy:  0.575115023005 train_accuracy:  0.716844444444\n",
      "Iter:  15700 loss:  35.4052881189 val_accuracy:  0.574114822965 train_accuracy:  0.719155555556\n",
      "Iter:  15750 loss:  35.416189207 val_accuracy:  0.573114622925 train_accuracy:  0.718977777778\n",
      "Iter:  15800 loss:  35.3467222909 val_accuracy:  0.573514702941 train_accuracy:  0.717733333333\n",
      "Iter:  15850 loss:  35.482065155 val_accuracy:  0.572114422885 train_accuracy:  0.717622222222\n",
      "Iter:  15900 loss:  35.4675886415 val_accuracy:  0.571514302861 train_accuracy:  0.717911111111\n",
      "Iter:  15950 loss:  35.4058922719 val_accuracy:  0.574314862973 train_accuracy:  0.718488888889\n",
      "Iter:  16000 loss:  35.4101912591 val_accuracy:  0.575515103021 train_accuracy:  0.718888888889\n",
      "Iter:  16050 loss:  35.3438936563 val_accuracy:  0.574314862973 train_accuracy:  0.719866666667\n",
      "Iter:  16100 loss:  35.4996182576 val_accuracy:  0.572514502901 train_accuracy:  0.718644444444\n",
      "Iter:  16150 loss:  35.4507049286 val_accuracy:  0.577915583117 train_accuracy:  0.719555555556\n",
      "Iter:  16200 loss:  35.4192356347 val_accuracy:  0.575115023005 train_accuracy:  0.719511111111\n",
      "Iter:  16250 loss:  35.4422949249 val_accuracy:  0.575115023005 train_accuracy:  0.719933333333\n",
      "Iter:  16300 loss:  35.5041591736 val_accuracy:  0.572114422885 train_accuracy:  0.720177777778\n",
      "Iter:  16350 loss:  35.4401795842 val_accuracy:  0.571114222845 train_accuracy:  0.719266666667\n",
      "Iter:  16400 loss:  35.386800031 val_accuracy:  0.575515103021 train_accuracy:  0.720577777778\n",
      "Iter:  16450 loss:  35.4143996513 val_accuracy:  0.573114622925 train_accuracy:  0.719377777778\n",
      "Iter:  16500 loss:  35.4812719474 val_accuracy:  0.574714942989 train_accuracy:  0.721511111111\n",
      "Iter:  16550 loss:  35.4690671224 val_accuracy:  0.573314662933 train_accuracy:  0.720666666667\n",
      "Iter:  16600 loss:  35.4914766282 val_accuracy:  0.575915183037 train_accuracy:  0.721222222222\n",
      "Iter:  16650 loss:  35.4256332032 val_accuracy:  0.573114622925 train_accuracy:  0.719644444444\n",
      "Iter:  16700 loss:  35.4003006267 val_accuracy:  0.572914582917 train_accuracy:  0.720555555556\n",
      "Iter:  16750 loss:  35.4351363877 val_accuracy:  0.574914982997 train_accuracy:  0.720577777778\n",
      "Iter:  16800 loss:  35.4359787357 val_accuracy:  0.573714742949 train_accuracy:  0.721977777778\n",
      "Iter:  16850 loss:  35.4459950611 val_accuracy:  0.573914782957 train_accuracy:  0.720555555556\n",
      "Iter:  16900 loss:  35.4349345939 val_accuracy:  0.577315463093 train_accuracy:  0.721088888889\n",
      "Iter:  16950 loss:  35.5214769032 val_accuracy:  0.576315263053 train_accuracy:  0.720288888889\n",
      "Iter:  17000 loss:  35.4691545635 val_accuracy:  0.573514702941 train_accuracy:  0.722555555556\n",
      "Iter:  17050 loss:  35.4455664582 val_accuracy:  0.571314262853 train_accuracy:  0.7222\n",
      "Iter:  17100 loss:  35.4818126906 val_accuracy:  0.574514902981 train_accuracy:  0.721444444444\n",
      "Iter:  17150 loss:  35.5187788805 val_accuracy:  0.574914982997 train_accuracy:  0.721755555556\n",
      "Iter:  17200 loss:  35.5187021058 val_accuracy:  0.575315063013 train_accuracy:  0.721088888889\n",
      "Iter:  17250 loss:  35.5560006463 val_accuracy:  0.575915183037 train_accuracy:  0.722977777778\n",
      "Iter:  17300 loss:  35.4808533733 val_accuracy:  0.573114622925 train_accuracy:  0.721977777778\n",
      "Iter:  17350 loss:  35.4916007836 val_accuracy:  0.575715143029 train_accuracy:  0.724844444444\n",
      "Iter:  17400 loss:  35.5730526261 val_accuracy:  0.574114822965 train_accuracy:  0.723377777778\n",
      "Iter:  17450 loss:  35.4101770587 val_accuracy:  0.573514702941 train_accuracy:  0.723222222222\n",
      "Iter:  17500 loss:  35.523482085 val_accuracy:  0.574114822965 train_accuracy:  0.723288888889\n",
      "Iter:  17550 loss:  35.5222161915 val_accuracy:  0.572914582917 train_accuracy:  0.723044444444\n",
      "Iter:  17600 loss:  35.5704857348 val_accuracy:  0.571714342869 train_accuracy:  0.724288888889\n",
      "Iter:  17650 loss:  35.528289399 val_accuracy:  0.575715143029 train_accuracy:  0.723533333333\n",
      "Iter:  17700 loss:  35.5954060308 val_accuracy:  0.576315263053 train_accuracy:  0.721266666667\n",
      "Iter:  17750 loss:  35.5113351003 val_accuracy:  0.573714742949 train_accuracy:  0.724488888889\n",
      "Iter:  17800 loss:  35.5189613875 val_accuracy:  0.572914582917 train_accuracy:  0.723755555556\n",
      "Iter:  17850 loss:  35.5758983266 val_accuracy:  0.574314862973 train_accuracy:  0.724488888889\n",
      "Iter:  17900 loss:  35.5929440443 val_accuracy:  0.573314662933 train_accuracy:  0.7244\n",
      "Iter:  17950 loss:  35.4804148545 val_accuracy:  0.574914982997 train_accuracy:  0.724\n",
      "Iter:  18000 loss:  35.4447130072 val_accuracy:  0.574714942989 train_accuracy:  0.726111111111\n",
      "Iter:  18050 loss:  35.4981065257 val_accuracy:  0.576515303061 train_accuracy:  0.725377777778\n",
      "Iter:  18100 loss:  35.5263843482 val_accuracy:  0.574514902981 train_accuracy:  0.723844444444\n",
      "Iter:  18150 loss:  35.5232617599 val_accuracy:  0.574314862973 train_accuracy:  0.724\n",
      "Iter:  18200 loss:  35.5294589883 val_accuracy:  0.572514502901 train_accuracy:  0.724022222222\n",
      "Iter:  18250 loss:  35.5372880582 val_accuracy:  0.572314462893 train_accuracy:  0.724311111111\n",
      "Iter:  18300 loss:  35.6121509822 val_accuracy:  0.574514902981 train_accuracy:  0.725577777778\n",
      "Iter:  18350 loss:  35.5466849986 val_accuracy:  0.572714542909 train_accuracy:  0.725\n",
      "Iter:  18400 loss:  35.5121638035 val_accuracy:  0.574114822965 train_accuracy:  0.724933333333\n",
      "Iter:  18450 loss:  35.554442397 val_accuracy:  0.571314262853 train_accuracy:  0.726577777778\n",
      "Iter:  18500 loss:  35.4750462287 val_accuracy:  0.572314462893 train_accuracy:  0.725666666667\n",
      "Iter:  18550 loss:  35.5195100878 val_accuracy:  0.572714542909 train_accuracy:  0.726155555556\n",
      "Iter:  18600 loss:  35.516255776 val_accuracy:  0.574714942989 train_accuracy:  0.726088888889\n",
      "Iter:  18650 loss:  35.614845967 val_accuracy:  0.571314262853 train_accuracy:  0.726555555556\n",
      "Iter:  18700 loss:  35.5272067085 val_accuracy:  0.570914182837 train_accuracy:  0.725777777778\n",
      "Iter:  18750 loss:  35.5470951949 val_accuracy:  0.573514702941 train_accuracy:  0.726444444444\n",
      "Iter:  18800 loss:  35.607782911 val_accuracy:  0.574314862973 train_accuracy:  0.727111111111\n",
      "Iter:  18850 loss:  35.5979799648 val_accuracy:  0.574114822965 train_accuracy:  0.726022222222\n",
      "Iter:  18900 loss:  35.5566829796 val_accuracy:  0.574714942989 train_accuracy:  0.725755555556\n",
      "Iter:  18950 loss:  35.6318513231 val_accuracy:  0.574114822965 train_accuracy:  0.727\n",
      "Iter:  19000 loss:  35.5672469954 val_accuracy:  0.574114822965 train_accuracy:  0.727377777778\n",
      "Iter:  19050 loss:  35.5859904722 val_accuracy:  0.572314462893 train_accuracy:  0.728111111111\n",
      "Iter:  19100 loss:  35.6330907527 val_accuracy:  0.576115223045 train_accuracy:  0.725688888889\n",
      "Iter:  19150 loss:  35.5272482608 val_accuracy:  0.574714942989 train_accuracy:  0.728511111111\n",
      "Iter:  19200 loss:  35.6405093878 val_accuracy:  0.571514302861 train_accuracy:  0.727222222222\n",
      "Iter:  19250 loss:  35.5943331188 val_accuracy:  0.573114622925 train_accuracy:  0.728177777778\n",
      "Iter:  19300 loss:  35.5778985539 val_accuracy:  0.576715343069 train_accuracy:  0.728133333333\n",
      "Iter:  19350 loss:  35.5552793193 val_accuracy:  0.573914782957 train_accuracy:  0.726355555556\n",
      "Iter:  19400 loss:  35.6753552766 val_accuracy:  0.572914582917 train_accuracy:  0.727222222222\n",
      "Iter:  19450 loss:  35.6218638459 val_accuracy:  0.572114422885 train_accuracy:  0.727888888889\n",
      "Iter:  19500 loss:  35.6486311268 val_accuracy:  0.572114422885 train_accuracy:  0.728866666667\n",
      "Iter:  19550 loss:  35.5950204234 val_accuracy:  0.571714342869 train_accuracy:  0.728755555556\n",
      "Iter:  19600 loss:  35.5745215649 val_accuracy:  0.573314662933 train_accuracy:  0.7274\n",
      "Iter:  19650 loss:  35.629709749 val_accuracy:  0.572714542909 train_accuracy:  0.7278\n",
      "Iter:  19700 loss:  35.7156269319 val_accuracy:  0.571714342869 train_accuracy:  0.727622222222\n",
      "Iter:  19750 loss:  35.5702962613 val_accuracy:  0.572514502901 train_accuracy:  0.728888888889\n",
      "Iter:  19800 loss:  35.6112203587 val_accuracy:  0.571114222845 train_accuracy:  0.729288888889\n",
      "Iter:  19850 loss:  35.7128482505 val_accuracy:  0.571714342869 train_accuracy:  0.730444444444\n",
      "Iter:  19900 loss:  35.5852528125 val_accuracy:  0.573114622925 train_accuracy:  0.728844444444\n",
      "Iter:  19950 loss:  35.6855618565 val_accuracy:  0.571914382877 train_accuracy:  0.729622222222\n",
      "best val accuracy:  0.577915583117 train accuacy at this point:  0.719555555556\n",
      "this is a test\n",
      "testing\n",
      "0.577915583117\n"
     ]
    }
   ],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0.2, reg_lambda=0.1)\n",
    "NN2.train(X_train, y_train, iters=20000, alpha=0.0001, batch_size=500, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    }
   ],
   "source": [
    "y_predicted2 = NN2.predict(X_test,'test')[1]\n",
    "save_predictions('ans2-id2303',y_predicted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  8545.71585608\n"
     ]
    }
   ],
   "source": [
    "t2 = time()\n",
    "print 'time taken: ',t2-t1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
